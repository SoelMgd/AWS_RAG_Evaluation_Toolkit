,level_0,question,RAGAS_context,ground_truth,RAGAS_source,evolution_type,index
0,0,What are some key points or best practices typically covered in secure coding guidelines for software development?,"["" these jobs. To do that, just run the pipeline in your GitLab Project > CI/CD > Pipeline > Run Pipeline > Select develop branch. And fill out the Variables: - AUTOM_NAME must be the identical to the value of the autom_name key of the dictionary you need to validate. - VALIDATE_DICTIONARY should be set to TRUE. :::\n\nPipeline\n\nNinGines Core\n\nAutomation projects are loaded with .gitlab-ci.yml, which includes NinGines core functionalities. A stage test or/and quality can be used with your own jobs in this CI to test your code (lint & quality check).\n\n:::caution You cannot define your own variables into this CI (variables: section) as it is already defined by the core. :::\n\nAutomation\n\nEach automation consists of a folder with a gitlab-ci file named automation.yml.\n\nThe automation folder name should be identical to the autom_name in the dictionary.\n\nAutomation description\n\n:::caution Shared runners must stay enabled in project settings to keep core NinGines jobs running. :::\n\nvariables: Load MMS variables in your automation.\n\njob:\n\nAUTOMATION_RESULT: Predefined and mandatory variable, set the value from 0 to 5 (see table below). Any other status must return a value of 1 as a failed result.\n\nAUTOMATION_RESULT NinGines status Cattask status 0 success Closed Complete 1 failed Open 2 cancelled Cancelled 3 Reserved for future use N/A 4 Reserved for future use N/A 5 Reserved for future use N/A\n\nAUTOMATION_WORKNOTE: Optional variable; to add a work note in the MMS Item when the request has failed\n\nAUTOMATION_COMMENT: Optional variable; to write a message (in the customer visible field)\n\nNinGines notify reference: Send automation result to MMS\n\nartifacts:\n\nIf you are not familiar with artifacts, please read the following documentation artifacts and needs\n\nAutomation customization\n\nUpdate the pipeline to fit your needs. You can use multiple stages, jobs, child pipeline and any feature supported by GitLab CI.\n\n:::tip NinGines always requires: - ningines-public included in the .gitlab-ci.yml file - extends: .ng:variables in variables section - One AUTOMATION_RESULT variable defined - One notify call only. :::\n\nAutomation examples\n\nTo interact with NinGines automation.yml requires some code base. Your automation project ship with two examples to quickly start playing with your automation pipeline:\n\nExample Description Basic The minimum code to get any automation working Callback Non linear execution with separate jobs\n\nDevelop your automation\n\nPrerequisites\n\nTo align with NinGines' policies, the following prerequisites must be met:\n\n:::info - Find and replace the request id in your dictionary. - If you are working on develop branch, the dictionary key mms_develop need to be less than 15 days away. - The Short Description of the MMS Cattask must match the cattask_short_description key in the dictionary. - The autom_name key of the dictionary must match the name of the automation folder in your repository. - NinGines will fetch Cattask item with the key enabled at true in the dictionary. - NinGines will fetch Cattask item only if it is not assigned to anyone. :::\n\nMMS variables\n\nIn the job autom:generate-config variables are passed from MMS with the following artifacts: - autom.gitlab-ci.yml: your file automation.yml update and rename by NinGines Core. - autom.variables: MMS variables in bash format. - autom.variables.json: MMS variables in JSON format. - dev_variables: MMS variables in bash format for dev purpose.\n\nHere an example of the variables file:\n\n:::note In addition of native variables for Cattask, you'll find the variable corresponding to your MMS form design. :::\n\nBranch strategy\n\nThe main branch always runs automation on the MMS PROD environment. Use the develop branch to test NinGines automation in MMS DEV and MMS TEST environments. Use any `` branch to test NinGines automation without MMS.\n\nOffline automation development\n\nRather than submit an MMS item to test automation, you can directly trigger your automation pipeline. 1. You have 2 options: - Raise your MMS item in MMS DEV and download the dev_variables file from any autom:generate-config job and upload it to your automation folder (must be the identical to the autom_name""]","Some key points and best practices typically covered in secure coding guidelines for software development include input validation, output encoding, authentication and access control, cryptography, error handling, data protection, secure communications, and keeping software up-to-date with the latest security patches and updates.","[{'source': 'cloud-services\\docs\\ningines\\03-interface\\01-request-1-7.md', 'filename': 'cloud-services\\docs\\ningines\\03-interface\\01-request-1-7.md'}]",simple,
1,1,What is the role and responsibility of the Technical Account Manager (TAM) in the AWS support team for Air Liquide?,"[""## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## AWS Team\n\nAWS Account Team\n\nAir Liquide is subscribed to the AWS Enterprise Support. It provides 24x7 technical support from high-quality engineers, tools and technology to automatically manage our environments, consultative architectural guidance delivered in the context of your applications and use-cases, and a designated Technical Account Manager (TAM) to coordinate access to proactive / preventative programs and AWS subject matter experts.\n\nAWS Role Name Contact email Contact Tel Scope Account Manager (AM) Edouard NOMME nomme@amazon.com +33 6 28 54 67 11 EMEA Technical Account Manager (TAM) RÃ©mi DURANCET dduranc@amazon.fr +33 (0) 7 71 44 41 06 EMEA Solutions Architect (SA) Olivier ROSSANT orossant@amazon.fr +33 (0) 6 88 13 51 13 EMEA Customer Solutions Manager (CSM) Omar ZRAIBI omarzr@amazon.fr +33 (0) 6 59 63 09 81 EMEA Conciergerie Ann HESTERMANN maryjeah@amazon.com - EMEA Central Sales Representative (CSR) Alexandre RUSCART ruscarta@amazon.com - EMEA Account Manager (AM) Donielle NYLAND ndoniell@amazon.com - AMER Solutions Architect Ewanlen ATIVIE mativie@amazon.com - AMER Customer Solutions Manager (CSM) Danilo LIBERZTO danliber@amazon.fr - AMER Account Manager Vimalraj SELVARAJ msvimalr@amazon.com - APAC Solutions Architect (SA) Keith ROZARIO krozario@amazon.com - APAC Business Developer (BD) Jingxian SUN suhingxi@amazon.com - China Solutions Architect (SA) Phil ZHAO hongz@amazon.com +86-137-6461-6419 China\n\nThe AWS account team can be reached via the following distribution lists: * AWS TAM (Global): aws-airliquide-tams@amazon.com * AWS team (EMEA): aws-airliquide-emea@amazon.com * AWS team (Global): aws-airliquide-emea@amazon.com\n\nAll AWS member have -sc@airliquide.com accounts and can be reached via Google Chat.\n\nHow to contact the team\n\nYou need support for general questions about services and architecture topics: open a case to the support and/or directly contact your region's SA and TAM\n\nYou have a question about the AWS Roadmap or want to submit a PFR: contact the TAM\n\nYou want more details about incentives, migration programs: contact your region's Account Manager\n\nYou have a question or issue about billing topics: contact the AWS Concierge\n\nYou need a follow up on operational and billing topics: contact teh Customer Solutions Manager\n\nYou have a P1 incident and need urgent support from AWS: open a ticket to the AWS support with the appropriate severity level. The TAM* will be paged and will onboard timely to coordinate the AWS people to work towards the resolution\n\nThe AWS Team has a Google Chat account and can be reached via this channel if necessary.""]",The Technical Account Manager (TAM) for Air Liquide's AWS account has the following role and responsibilities: - Coordinate access to proactive/preventative programs and AWS subject matter experts - Provide consultative architectural guidance in the context of Air Liquide's applications and use-cases - Act as the primary point of contact for questions about the AWS Roadmap or submitting Product Feature Requests (PFRs) - Get involved and coordinate AWS resources in case of P1 (critical) incidents to work towards resolution,"[{'source': 'cloud-services\\docs\\aws-lab-essential\\01-intro\\04-aws-team.md', 'filename': 'cloud-services\\docs\\aws-lab-essential\\01-intro\\04-aws-team.md'}]",simple,
2,2,What are the key steps involved in setting up TLS in AWS ACM (AWS Certificate Manager)?,"["" one domain you don't need to provide an attachment.\n\nRequest an internal certificate\n\n:::info\n\nSee the detailed tutorial on this topic\n\n:::\n\nSet up TLS in AWS ACM\n\nUpload your certificate to AWS\n\nTo upload your certificate to AWS ACM either via the Console or the AWS CLI, please follow the official AWS ACM User Guide\n\n:::caution\n\nNote the ARN of your certificate. It will be used as a reference in the next step.\n\n:::\n\nAdd your certificate to your Load Balancer\n\nTest\n\nOnce all steps are complete:\n\nfrom the Windows CMD or Linux shell, use nslookup myapp.apps.airliquide.com. It should return the record set myapp.123456789012.eu.aws.airliquide.com and the IPs of the ALB starting with 10.151.x.y\n\nopen Chrome and browse to your application https://domain. If your web server is up and running, you should see your application.\n\nExpose your web application on the internet\n\nIf you have followed the steps above, your application is now available from the Air Liquide Network. Now, you may need to expose your web application on the internet. In addition to the steps described below, you will need to request the implementation of the Web Application Firewall (WAF) and make your DNS publicly available.\n\n:::info\n\nSubmit an Open Demand in ManageMyService\n\n:::\n\nThe implementation of the WAF will be delivered by GIO/PSS team who will get in touch with you:\n\nExposition of the application on the public internet via the WAF (the public IP of the application is set on the WAF)\n\nMaking your DNS publicly accessible\n\nCreation and implementation of the public certificate for your domain(s)\n\nSetup of the network connection between the WAF and your VPC(s)\n\nSupport during the learning phase, where you will work with PSS to configure the rules on the WAF""]","The key steps involved in setting up TLS in AWS ACM (AWS Certificate Manager) based on the given context are:

1. Request an internal certificate
2. Set up TLS in AWS ACM
3. Upload your certificate to AWS ACM either via the Console or the AWS CLI
4. Add your certificate to your Load Balancer
5. Test the setup by using nslookup and browsing to the application URL","[{'source': 'cloud-services\\docs\\aws-lab-essential\\08-tutorials\\05-application-deployment\\01-tuto-app.md', 'filename': 'cloud-services\\docs\\aws-lab-essential\\08-tutorials\\05-application-deployment\\01-tuto-app.md'}]",simple,
3,3,What is the significance of the 'Operations' email address in the 'Alternate Contacts' section of an AWS account?,['\n\nI own the AWS account on which the IAM User is. Why am I not receiving any mail? It means that the âOperationsâ email address into âAlternate Contactsâ of your AWS account is not properly configured.\n\nCan I have more information about IAM User usage/process? Visit this section of this documentation.'],"The 'Operations' email address in the 'Alternate Contacts' section of an AWS account is important for receiving notifications and alerts related to the account. If this email address is not properly configured, the user may not receive important communications from AWS, such as billing alerts or service updates.","[{'source': 'cloud-services\\docs\\aws-lab-essential\\04-landing-zone-references\\07-autom-ref\\01-iam-user-key.md', 'filename': 'cloud-services\\docs\\aws-lab-essential\\04-landing-zone-references\\07-autom-ref\\01-iam-user-key.md'}]",simple,
4,4,What is the purpose of the Virtual Private Server offer?,"[""## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## Virtual Private Server\n\nPurpose of this offer\n\nThis offer exists to deploy simple applications on Virtual Machines (VM) (EC2) at AWS which requires only some VMs (EC2) with storage and when a dedicated AWS account is not needed.\n\nIf you need an architecture more than simple(s) VM/ EC2 for your application, you need to request a dedicated AWS account (Managed or Essential) and deploy the needed infrastructure in it.\n\nThis offer can be used to deploy some VMs (EC2) at AWS in an GIO's owned and managed account for IT or for OT (OT only for the BIS concerned): the deployment is done in a different account for IT and OT.\n\nPricing\n\nYou pay your consumption of EC2 instance, no fix cost.""]","The purpose of the Virtual Private Server offer is to deploy simple applications on Virtual Machines (VMs) or EC2 instances at AWS, which requires only some VMs with storage and when a dedicated AWS account is not needed.","[{'source': 'cloud-services\\docs\\virtual-private-server\\02-purpose\\index.md', 'filename': 'cloud-services\\docs\\virtual-private-server\\02-purpose\\index.md'}]",simple,
5,5,What is the purpose of running the pipeline and providing the automation folder name as an input variable in the CI/CD process?,"[':\n\nManually run the pipeline in your GitLab Project > CI/CD > Pipeline > Run Pipeline > Select develop or ` branch, then fill out theInput Variable valuematching with your automation folder name for the variableAUTOM_NAME`.\n\nDevelopment rules and standards\n\nAll your development - no matter the hosting platform and whatever language you use - need to use secure protocols and respect software development best practices. You can rely on the relevant documentation & guidelines within the Air Liquide Digital Security framework:\n\nSecure Coding guidelines\n\nShift Left and DevSecOps principles\n\nIf you have any question or doubt regarding the software development in Air Liquide, please get in touch why the CoP Software Engineering:\n\nCurrents\n\nww-cop-software-engineering-core-team@airliquide.com\n\nSecurity recommendations\n\nDo not :x::\n\nStore in your code any sensitive data (token, key, password).\n\nUse a runner calling a service/generic account with too many rights.\n\nDo :heavy_check_mark::\n\nRotate SSH keys and Personal Access Tokens.\n\nRestrict the access and rights of the service/generic account called by the runner.\n\nRely on AWS Secrets Manager or CyberArk (GIO/ADAS team) to store any credentials.\n\nReview your runner permissions.\n\nUsers rights management\n\nPlease refer to the paragraph Users rights management in the documentation Quick Start for NinGines.\n\nFAQ\n\nQ: Is it possible to get and use MMS Item attached files in my automation?\n\nNo. MMS User is not able to get the file attached in MMS items. Nevertheless, if you need to manage files in your automation, you can ask in the request to drop the file on cloud storage (Google Drive, AWS S3, Azure Storage).\n\nQ: Does my automation interact with servers hosted in the European Data Centers through SSH or WinRM?\n\nYes. Please read this documentation.\n\nTroubleshooting guide\n\nNinGines do not fetch my automation in MMS DEV or MMS TEST environment\n\nCheck your GitLab pipeline, you should have error or warning messages thanks to validate dictionary jobs.\n\nNinGines do not fetch my automation in any MMS environment\n\nCheck that the dictionary assignment group matches the MMS assignment group.']","The purpose of running the pipeline in the GitLab Project's CI/CD and providing the automation folder name as an input variable (AUTOM_NAME) is to trigger the execution of the automation code or scripts stored in the specified folder. This allows the automation to be run as part of the Continuous Integration and Continuous Deployment (CI/CD) process, enabling automated testing, building, and deployment of the automation code.","[{'source': 'cloud-services\\docs\\ningines\\03-interface\\request-1-6.md', 'filename': 'cloud-services\\docs\\ningines\\03-interface\\request-1-6.md'}]",simple,
6,6,What resources need to be manually cleaned up after terminating the Service Catalog product for EBS volume encryption?,"["" NORMAL All non-encrypted EBS volumes attached to the instances can be encrypted The encryption operations will be done on all EBS volumes EXCLUDE All EBS volumes attached to the instance are already encrypted. No operation needed. WARNING PARTIAL SOME EBS volumes attached to the instance cannot be encrypted. Check the EBS volumes details in the logs for more explanations. The encryption operations will be done on EBS volume which can be encrypted WARNING EXCLUDE ALL EBS volumes attached to the instance cannot be encrypted. Check the EBS volumes details in the logs for more explanations. The EBS encryption operations can't be executed\n\nCleaning\n\nWhen you don't need anymore to execute the encrypt operation, just Terminate the product from Provisioned products list, into Service Catalog console.\n\nWhen you terminate the product, the following resources are deleted:\n\nAll resources created by the product (Lambda functions, Step Functions, IAM Roles, ...)\n\nLambda functions log stream.\n\nFor the other resources, the cleaning will be done automatically after a retention period or in some cases, you need to do cleaning by your own:\n\nLog streams into operation log group are not deleted. They will be automatically cleaned up at the end of the specified retention period (default = 30 days).\n\nAMI (and so snapshots) used as backup during the operation. To automatically clean it, the DeleteOn tag is setup on AMIs. By default, an AMI is deleted after 30 days.\n\n:::caution\n\nAs the automated AMI clean up is not deployed by default in Lab accounts, no cleaning will be done.\n\n:::\n\nOld non-encrypted EBS Volumes. If, into the product's parameters, you decided to not delete them, those volumes will remain and will never be automatically deleted. You will have to delete them manually yourself. To help you to spot them, the following tags can be looked up in the EBS console, via CLI or API:\n\nTag Value EBSEncryption/status done EBSEncryption/newVolume id of newly created volume""]","The resources that need to be manually cleaned up after terminating the Service Catalog product for EBS volume encryption are the old non-encrypted EBS volumes, if the option to not delete them was chosen in the product's parameters. These volumes will remain and will never be automatically deleted, so they need to be deleted manually by looking up the tags 'EBSEncryption/status' with value 'done' and 'EBSEncryption/newVolume' with the ID of the newly created volume. AMI (and so snapshots) used in backup are not deleted by default in Lab accounts as the automated AMI clean up is not deployed. Moreover, log streams into operation log group are not deleted but will be automatically cleaned up at the end of the specified retention period (default = 30 days).","[{'source': 'cloud-services\\docs\\aws-lab-essential\\07-service-catalog\\03-ebs_encryption.md', 'filename': 'cloud-services\\docs\\aws-lab-essential\\07-service-catalog\\03-ebs_encryption.md'}]",simple,
7,7,What AI-powered view flags accounts with cost anomalies for review?,"["" cost in. All. Set to current month only in the Summary and Cost Optimization views. Business Application Allows you to filter per application and application environment (requires that your application exists in MMS and is linked with its AWS accounts). All Client Filter the cost for one or more clients (i.e BIS, GSP) All AWS Account Select one ore more AWS Accounts to see the associated cost All AWS Service Select one ore more AWS service (i.e. EC2, RDS, LAMBDA...) to see the associated cost. Note that ES stands for Amazon ElasticSearch Service All GIO account offering Filter by GIO AWS account offering (ie: Lab, Essential, Managed) All Charge Type Select the type of charge you want to include in your chart (i.e. credits, refunds, taxes...). TAX is always unselected by default All Resource ID Filter per resource (i.e EC2, RDS instances...) identified as optimizable Cost Optimization only Trusted Advisor Check Name Filter per Trusted Advisor check Cost Optimization only GIO Billing period Select the GIO billing period. Note that MR for month N contains AWS billing data from month N-2 due to AWS updating billing data 15 days after month closure Monthly Report only GIO Service ID Filter by the service ID presented to you in GIO Monthly Reports Monthly Report only\n\nThe top-right HUB filter relates to GIO Hubs (Americas, Asia-Pacific and Europe + Middle-East Africa).\n\nGroup By and Aggregation slicers\n\nThe dashboard offers you Group By and Aggregate By slicers. You can combine them with other specific filtering options.\n\nSlicer Scope Available views Group By View the spend per the proposed categories (AWS account, AWS service, Hub...) Aggregate By Select the financial view you need (see details below this table)\n\nDetails for the Aggregate By:\n\nUnblended Cost: This is what AWS actually charges Air Liquide every month. Itâs the default option for analyzing costs.\n\nTotal Billed GIO: This is what is charged to you by GIO via Monthly Reports. It includes all Savings Plan and Enterprise Support cost, that are not shown with Unblended Cost. Amounts are applicable for Hub EUR only.\n\nAmortized Cost: View the amortized costs - typically useful if you have paid upfront fees for reservations for example. With this view, the total upfront amount is spread across the amortization period\n\nSavings Plan cost: the share of GIO-managed Savings Plans (shared) that is charged back to you based on your Compute usage\n\nEnterprise Support cost: your share of the AWS Premium Support cost invoiced to Air Liquide by GIO (more details here)\n\nViews\n\nSummary\n\nIt shows a snapshot of your cost for the current month only. It allows to see quickly the Month-to-date (MTD) and Year-to-date (YTD) cost, as well as a forecasted value for the month-end (simple, linear projection). The forecasted value is not a commitment from GIO or AWS ; your actual bill may differ. The top-right chart shows you how much you would have paid if GIO hadn't managed cost optimizations globally.\n\nBill Explorer\n\nThe Bill Explorer view allows you to easily analyze your AWS bill using groupings. It shows the data for the current month. There is no 2-month offset like in the Monthly Report view.\n\n:::caution\n\nPower BI fails to display all the items in a bar chart when there are too many. Therefore, the views grouped by Account and Service won't show all items and the presented cost will reflect only the items that are showed on the chart. To have an accurate cost, please use the filters on the left to reduce the scope. (Blank) is displayed for AWS Accounts that are new and still in status transition in MMS at the time the report data is collected.\n\n:::\n\nVariance\n\nThe Variance view allows you to easily visualize which Service, Account, etc... contributes to the evolution (increase or decrease) of your AWS cost. The table allows you to access the detailed data for your selected scope and timeline.\n\nCost drifts\n\nThis view shows at a glance the cost anomalies at AWS account and service level detected by an AI model integrated by GIO. The detailed view can help you visualize your alerts per service, to understand the cost impact and investigate further if needed to stop the cost deviation.\n\nAn anomaly is defined as an abnormal cost trend at the AWS account level, that can be interpreted as either an Alert or a Warning.\n\nALERT: there is an anomaly that needs immediate attention\n\nWARNING: there was an anomaly but the cost trend went back to normal\n\nAccounts details\n\nThis view presents information""]","The 'Cost drifts' view shows cost anomalies at the AWS account and service level detected by an AI model, flagging accounts with abnormal cost trends as alerts or warnings for further investigation.","[{'source': 'cloud-services\\docs\\aws-lab-essential\\05-finops\\07-explore.md', 'filename': 'cloud-services\\docs\\aws-lab-essential\\05-finops\\07-explore.md'}]",reasoning,
8,8,Which LZI components face impact across regions during a Main HUB region outage?,"[""AWS Landing Zone Resilience consideration\n\nimport Badge from 'react-bootstrap/Badge';\n\nIn this section we explain, for each most important pieces of AWS Landing Zone, resilient regarding major issue like an AWS data center lost. What will continue to work, what will not?\n\n:::info\n\nNote we will use LZI word above to designate the AWS Landing Zone (i.e. Landing Zone Infrastructure)\n\n:::\n\nHow read the below tables: - Component = Which LZI component we are talking about - Function Run = It is the operational status of deployed LZI component, how it is usable regarding the potential issue we have. In other word, can my application continue to use this LZI component during the issue without any impact. - Function Deploy = It is the capability to build/deploy/update the LZI component during the issue. - AZ Trouble/Lost = AWS Availability Zone issue. In the worse case, it can be the loss of an entire AWS Data center. It is the main risk we have with a long term impact. In other word, it the risk we must absolutely cover. - Main HUB region Trouble/Lost = Troubles consideration related to the Main region use by the LZI. You can find list of Main LZI regions here. - Other region Trouble/Lost = Troubles consideration related to the other regions than Main region use by the LZI. - Services Used = Main services used for this component of LZI. Basically, resilience of the component is lower or equal to the worst services in the list.\n\nInfra as code\n\nOne of the first considerations that we should have is that LZI is globally written with Infra As Code and therefore depends on the resilience of the software factory such as Gitlab. In other words, LZI code and pipelines have the same level of resilience than Software factory, and in particular Gitlab application.\n\n\u200bComponent Function AZ Trouble/Lost Main HUB region Trouble/Lost Other region Trouble/Lost Services Used Comment LZI Code Run No Impact Trouble Impact into region Software Factory Run are note directly affected for issue related to code repository. However some regular schedule of CD pipeline can be affected, but there is no direct impact in this case on on going usage of LZI. \u200b Deploy No Impact (+ eu-west-1 region - Ireland)Impacted (- eu-west-1 region - Ireland)Impact into region Software Factory In case of eu-west-1 (Ireland) region troubles, build/deploy/update of the LZI can be compromised because it is the software factory region. For Main HUB region other than Ireland, software factory will work, but not the LZI Cockpit of corresponding HUB.Other deployment components are build on multiple AZ architecture, so without any impact in case of AZ trouble.\n\nIAM\n\nIn this section, we will consider all standard IAM components deployed by the LZI into all accounts.\n\n\u200bComponent Function AZ Trouble/Lost Main HUB region Trouble/Lost Other region Trouble/Lost Services Used Comment IAM Roles/Policies Run No Impact No Impact Impact into region IAM Use AWS IAM IAM services which is a global service resilient in case of region loss. \u200b Deploy No Impact Impacted No Impact CloudFormation Component deployed using cloudformation stackset from the Cockpit account. See Cockpit section for more information. IAM Automation (Boundaries,...) Run No Impact (+ us-east-1 region - N. Virginia)Trouble No Impact IAM Lambda SSM EventBridge IAM Automation use the us-east-1 region. So those automation are impacted in case of us-east-1 region issue. But this component is not critical for application. For example, assign boundary policy failure, will not block usage of an IAM roles for this application, and after issue, automatically, IAM Automation will retry the backlog. \u200b Deploy No Impact (+ us-east-1 region - N. Virginia)Impacted No Impact CloudFormation Component deployed using cloudformation stackset from the Cockpit account. See Cockpit section for more information.\n\nCockpit\n\nCockpit is a special AWS account from which LZI is managed and deployed.\n\n\u200bComponent Function AZ Trouble/Lost Main HUB region Trouble/Lost Other region Trouble/Lost Services Used Comment CloudFormation StackSet Run No Impact No Impact No Impact N/A The CloudFormation stackset is used for the LZI into the Cockpit only for deployment not for run. So no impact on run. \u200b Deploy No Impact Impacted Impact into region CloudFormation CloudFormation stackset is one of the main component used to deploy the LZI. It is a regional service, so it is impacted during a regional issue on main LZI region. Furthermore this service is not impacted by an AZ issue. LZI CLI Server Run No""]","The following LZI components face impact across regions during a Main HUB region outage: CloudFormation StackSet (Deploy), LZI CLI Server (Run and Deploy), and IAM Automation (Deploy).","[{'source': 'cloud-services\\docs\\aws-lab-essential\\03-architecture-and-concepts\\11-lzi-res.md', 'filename': 'cloud-services\\docs\\aws-lab-essential\\03-architecture-and-concepts\\11-lzi-res.md'}]",reasoning,
9,9,Which initiative propagates policies to the mgmt. group with allowed locations NE & USE2?,"["" Location\n\nAllowed locations : North Europe, US East 2, Southeast Asia, Australia East\n\nCategory: Storage\n\nAudit Prod Storage Account Sku\n\nAudit Non-Prod Storage Account Sku\n\nAzure DYN Initiative\n\nCategory: Tags\n\nInherit ProjectName tag from the subscription\n\nInherit ProjectShort tag from the subscription\n\nInherit CustomerName tag from the subscription\n\nInherit EnvironmentName tag from the subscription\n\nInherit CostCenter tag from the subscription\n\nCategory: Location\n\nAllowed locations : Canada Central, France Central, North Europe\n\nCategory: Virtual Machine\n\nMicrosoft IaaSAntimalware extension should be deployed on all servers\n\nCategory: Virtual Network\n\nInternet-facing virtual machines should be protected with network security groups\n\nNon-internet-facing virtual machines should be protected with network security groups\n\nInitiatives and Managements groups\n\nAzure Core Initiative is propagated in Azure CORE Management Group\n\nAzure BIG Initiative is propagated in Azure BIG Management Group\n\nAzure DYN Initiative is propagated in Azure DYN Management Group\n\nAzure Core Reporting\n\nReporting data from Azure platform helps us to monitor infrastructure state. Configuration compliance could be checked using operational data.\n\nExpressRoute\n\nEurope\n\nExpressRoute lets Air Liquide extend its on-premises networks into the Microsoft cloud over a private connection with dedicated bandwidth from a connectivity provider. In the Air liquide context, ExpressRoute is mostly used to reach Microsoft public services such as Azure AD, Azure SQL Database, Azure Storage, etc. Nevertheless, an Azure Private peering is available to establish a connection with private networks on Azure if it is necessary in future scenarios. A virtual cross-connection through a connectivity provider at a co-location facility offers more reliability, faster speeds, consistent latencies, and higher security than typical connections over the Internet to Air Liquide.\n\nThe European Expressroute circuit is hosted on the subscription GIO-CORE_SERVICES.\n\nAll the resources related to the European ExpressRoute circuit are deployed in the North Europe region:\n\nResource group: al-neu-core-prod-rgp-expressroute\n\nExpressRoute circuit: al-neu-core-prod-ercircuit-london\n\nProvider: Level 3 Communications - Exchange\n\nPeering Location: London\n\nBandwidth: 200 Mbps\n\nSKU: Standard\n\nBilling model: Metered\n\nTwo peerings are active on this ExpressRoute circuit:\n\nAzure Private:\n\nPrimary subnet: 169.254.254.180/30\n\nSecondary subnet: 169.254.254.176/30\n\nVLAN ID: 176\n\nConnection: al-neu-core-prod-cn-alnetwork-to-vnet-hub-vngtw-01\n\nVirtual Network: al-neu-core-prod-vnet-hub\n\nVirtual Network Gateway: al-neu-core-prod-vnet-hub-vngtw-01\n\nSKU: Standard\n\nGateway type: ExpressRoute\n\nPublic IP: 40.69.26.54\n\nMicrosoft:\n\nPrimary subnet: 185.188.176.40/30\n\nSecondary subnet: 185.188.176.44/30\n\nIpv4 Advertised public prefixes: 185.188.176.0/22\n\nVLAN ID: 40\n\nRoute filters\n\nA route filter lets us identify services we want to consume through our ExpressRoute circuits' Microsoft peering. It is essentially a list of all allowed BGP community values. Once a route filter resource gets defined and attached to an ExpressRoute circuit, all prefixes that map to the BGP community values are advertised to Air Liquide's network.\n\nRoute filter name: al-neu-core-prod-ercircuit-london-rf\n\nRule name: al-neu-core-prod-ercircuit-london-rf-rule-01\n\nAllowed service communities:\n\nName Value Azure North Europe 12076:51003 Azure Global Services 12076:5050 Azure West Europe 12076:51002 Azure Active Directory 12076:5060\n\nAir Liquide Network Architecture\n\n:::info\n\nWaiting for documentation from Network Team\n\n:::\n\nContacts\n\nIf there is any issue with the European ExpressRoute configuration, besides the Azure Infra Team, it is possible to reach GIO Network Team, specifically Karim Rahmani(karim.rahmani-sc@airliquide.com).\n\nAsia\n\nThe Asian Expressroute circuit is hosted on the subscription""]",The 'Azure DYN Initiative' propagates policies to the management group with allowed locations North Europe and US East 2.,"[{'source': 'cloud-services\\docs\\azure\\01-core-platform.md', 'filename': 'cloud-services\\docs\\azure\\01-core-platform.md'}]",reasoning,
10,10,How does NinGines enable auto-remediation via GitLab pipelines for incidents & reqs?,"[""Incident v1.7\n\nimport VersionDropDown from '@site/src/components/VersionDropDown'; import VersionData from './version.json';\n\nThis section provides instructions on how to use NinGines to catch incidents and trigger appropriate automation in the GitLab pipeline, enabling you to update the ticket status according to the status of the automation.\n\nHow to start\n\nRequest your NinGines GitLab project following the Quick start.\n\nRead this documentation. :bookmark_tabs:\n\nFollow the prerequisites and the branch strategy.\n\nUpdate your dictionary.\n\nSubmit your MMS item in MMS DEV to get variables and play with dev.variables file.\n\nDevelop your automation. :rocket:\n\nTriggers\n\n:::info NinGines Core fetches Incident items every 5 minutes in all MMS environments. :::\n\n:::tip The MMS Incident structure is under the responsibility of your team. For further information, reach out to teams managing automatic incident creation, like Splunk or Prisma Teams. :::\n\nDictionary\n\nDictionaries permit to match MMS items with NinGines pipelines. In your GitLab project you'll find a dictionary file in dictionaries folder. Here is the dictionary structure for the Incident items:\n\nDescription\n\nincident_short_description: MMS incident short description.\n\nautom_name: Automation name.\n\nenabled: true or false.\n\ncomment: Comment for your personal use.\n\n:::note - A dictionary is only for one assignment group. - Multiple dictionaries are supported. Please open a new request per assignment group and select the same GitLab group and GitLab project. - The dictionary header must remain untouched. - Add a new automation section to handle more than one automation in the same dictionary. :::\n\nValidate dictionary jobs\n\nSeveral jobs run when you modify and push your dictionary on your develop branch: - test:dictionary: this job will exit with a failed status if your dictionary doesn't conform to the specified JSON schema of your MMS item. - validate:auto-disable-date-value: this job will exit with a failed status when the auto_disable_date key is more than 15 days from now or in the past. - validate:enabled-values: this job will exit with a warning status when the value of all the enabled keys are set to false.\n\n:::tip In addition, if you have some doubt about one of your dictionaries, you can manually run these jobs. To do that, just run the pipeline in your GitLab Project > CI/CD > Pipeline > Run Pipeline > Select develop branch. And fill out the Variables: - AUTOM_NAME must be the identical to the value of the autom_name key of the dictionary you need to validate. - VALIDATE_DICTIONARY should be set to TRUE. :::\n\nPipeline\n\nNinGines Core\n\nAutomation projects are loaded with .gitlab-ci.yml, which includes NinGines core functionalities. A stage test or/and quality can be used with your own jobs in this CI to test your code (lint & quality check).\n\n:::caution You cannot define your own variables into this CI (variables: section) as it is already defined by the core. :::\n\nAutomation\n\nEach automation consists of a folder with a gitlab-ci file named automation.yml.\n\nThe automation folder name should be identical to the autom_name in the dictionary.\n\nAutomation description\n\n:::caution Shared runners must stay enabled in project settings to keep core NinGines jobs running. :::\n\nvariables: Load MMS variables in your automation.\n\njob:\n\nAUTOMATION_RESULT: Predefined and mandatory variable, set the value from 0 to 5 (see table below). Any other status must return a value of 1 as a failed result.\n\nAUTOMATION_RESULT NinGines status Incident status 0 success Closed Complete 1 failed Open 2 Reserved for future use N/A 3 Reserved for future use N/A 4 Reserved for future use N/A 5 Reserved for future use N/A\n\nAUTOMATION_WORKNOTE: Optional variable; to add a work note in the MMS Item when the request has failed\n\nAUTOMATION_COMMENT: Optional variable; to write a message (in the customer visible field)\n\nNinGines notify reference: Send automation result to MMS\n\nartifacts:\n\nIf you are not familiar with artifacts, please read the following documentation artifacts and needs\n\nAutomation customization\n\nUpdate the pipeline to fit your needs. You can use multiple stages, jobs, child pipeline and any feature supported by GitLab CI.\n\n:::tip"", ""Request v1.7\n\nimport VersionDropDown from '@site/src/components/VersionDropDown'; import VersionData from './version.json';\n\nThis section provides instructions on how to use NinGines to catch requests and trigger appropriate automation in the GitLab pipeline, enabling you to update the ticket status according to the status of the automation.\n\nHow to start\n\nRequest your NinGines GitLab project following the Quick start.\n\nRead this documentation. :bookmark_tabs:\n\nFollow the prerequisites and the branch strategy.\n\nUpdate your dictionary.\n\nSubmit your MMS item in MMS DEV to get variables and play with dev.variables file.\n\nDevelop your automation. :rocket:\n\nTriggers\n\n:::info NinGines Core fetches Request items every 15 minutes in all MMS environments. :::\n\n:::tip MMS items development and management is under the responsibility of your team. For more information, please reach PCI team. When the MMS Item already exists, ensure that the form fits the automation needs. Keep it simple: - Fewer variables, the better. - Favor variables like Reference, Check Box, Select Box rather than Single Line Text to master customer choices. :::\n\nDictionary\n\nDictionaries permit to match MMS items with NinGines pipelines. In your GitLab project you'll find a dictionary file in dictionaries folder. Here is the dictionary structure for the Request items:\n\nDescription\n\nrequest_name: MMS request name.\n\nrequest_id: MMS request ID.\n\ncattask_short_description: MMS cattask short description.\n\nautom_name: Automation name.\n\nenabled: true or false.\n\ncomment: Comment for your personal use.\n\nHow to find the request ID\n\nFor basic requests, the ID is easily visible in the URL when you open a request. Example: https://airliquide.service-now.com/nav_to.do?uri=%2Fcom.glideapp.servicecatalog_cat_item_view.do%3Fv%3D1%26sysparm_id%3Daa0aefb91b436050510c7db4464bcbaa%26sysparm_link_parent%3D68dfaeab4f062f009864ca1f0310c724%26\n\n:::info Request ID: aa0aefb91b436050510c7db4464bcbaa :::\n\nFor Order Guide requests, the ID is easily visible in the URL when you open a request. Example: https://airliquide.service-now.com/nav_to.do?uri=%2Fcom.glideapp.servicecatalog_cat_item_guide_view.do%3Fsysparm_guide%3D35de9eb71bf4f010463b5205604bcb3a%26sysparm_active %3Deff226e74fdf0a003d9524fe0310c7b7%26sysparm_catalog%3D4a1e666b4f062f009864ca1f0310c7be%26sysparm_catalog_view%3Dcatalog_GIO_Services%26sysparm_view%3Dtext_search%26sysparm_cart_name%3D35de9eb71bf4f010463b5205604bcb3a\n\n:::info Request ID: eff226e74fdf0a003d9524fe0310c7b7 :::\n\n:::note - A dictionary is only for one assignment group. - Multiple dictionaries are supported. Please open a new request per assignment group and select the same GitLab group and GitLab project. - The dictionary header must remain untouched. - Add a new automation section to handle more than one automation in the same dictionary. :::\n\nValidate dictionary jobs\n\nSeveral jobs run when you modify and push your dictionary on your develop branch: - test:dictionary: this job will exit with a failed status if your dictionary doesn't conform to the specified JSON schema of your MMS item. - validate:auto-disable-date-value: this job will exit with a failed status when the auto_disable_date key is more than 15 days from now or in the past. - validate:enabled-values: this job will exit with a warning status when the value of all the enabled keys are set to false.\n\n:::tip In addition, if you have some doubt about one of your dictionaries, you can manually run""]","NinGines enables auto-remediation for incidents and requests via GitLab pipelines in the following way:

1. It fetches incidents or requests from the MMS (ServiceNow) system periodically.

2. Dictionaries are used to match MMS items with NinGines pipelines and automations. The dictionary contains details like the MMS item name, ID, automation name, and whether it is enabled or not.

3. When a matching MMS item is found, the corresponding automation defined in the GitLab project is triggered.

4. The automation is executed via jobs defined in the GitLab pipeline (automation.yml file). These jobs can perform various remediation actions.

5. The automation sets a predefined AUTOMATION_RESULT variable based on the outcome (success, failed, etc.), which updates the status of the MMS item accordingly.

6. Optionally, work notes or comments can be added to the MMS item based on the automation execution.

So in summary, NinGines acts as a bridge between MMS incidents/requests and GitLab pipelines, enabling automated remediation workflows to be executed and the MMS item status to be updated based on the automation result.","[{'source': 'cloud-services\\docs\\ningines\\03-interface\\02-incident-1-7.md', 'filename': 'cloud-services\\docs\\ningines\\03-interface\\02-incident-1-7.md'}, {'source': 'cloud-services\\docs\\ningines\\03-interface\\01-request-1-7.md', 'filename': 'cloud-services\\docs\\ningines\\03-interface\\01-request-1-7.md'}]",multi_context,
11,11,What are the key config opts for customizing patch levels & schedules for OS & apps using this soln?,"[""## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## OS Patching - Calendar synced\n\nOverview\n\nThis product deploys a patching solution that deploys patch updates on both the operating system and application levels using AWS SSM Maintenance Window it offers offers multiple levels of patching for both the operating system and application levels, providing you with flexibility and control over your patching strategy. This product catalog add the capability to deploy additional baselines at different schedules\n\n:::info\n\nOnce this product is deployed, you will have to add the Patch Group & Patch Schedule tags to your instances according to outputs :::\n\nPatching level\n\nThe classification and severity of the patch baseline are defined based on the operating system being patched. The following criteria are used to define the patch classification and severity:\n\nTwo criteria are used to determine the patching level : - Classification - Severity\n\nThe primary focus regarding patching is to provide operating systems security-related updates on managed nodes. This what we provide with Level1-Managed, the higher the patching level, the more patches will be applied to your system\n\nYou can chose your patching level according to the array below\n\nLevel Description LEVEL1-Managed LEVEL1-Managed is minimum requirement and refer to currently deployed managed baselineIt includes critical security patches that are necessary for your system's stability and security. LEVEL2 Includes Level1 + additional non security updates and low severity patches LEVEL3 Includes Level2 + additional features updates and recommended updates LEVEL4 Includes Level3 + additional upgrades, or newpackage LEVEL5-All This is the highest patching level and includes all available patches for your operating system and applications\n\nPatch Manager uses the severity level reported by the software publisher Exhaustive list of available properties can be find using DescribePatchProperties AWS System manager api call\n\nSchedule\n\nThe product allow you to patch your : - non production environment the saturday following the second tuesday of the month - production environment : up to 3 weeks after non production at the day of the week and hour of your choice\n\n:::important\n\nPlease not that it is important to apply patching in non production environment first, assess and validate, before deploy in production environment\n\n:::\n\nSupport\n\nThis template is supported by GIO Cloud Services. If you have any troubles, please open a ticket in myITCorner.\n\nCost\n\nDeploying this product doesn't incur charges.\n\nHow it works\n\nThe global behavior is the same than our currently deployed patching stack\n\nResources created\n\nThe product deploys the following resources: - An EventBridge Schedule rule to launch the lambda every month in order to always be synced with the second tuesday - A Lambda function and the necessary IAM role and to adjust the schedule based on the calendar - A maintenance window (and it's dependencies): This the when will patching occur - A patch baseline (and it's dependencies) : This is the what will be patched - An SNS Topic : You can subscribe in order to receive notifications when patching occurs\n\nConsideration\n\nWe recommend to deploy one product per environment and per operating system.\n\n:::tip\n\nIf you have one application composed of - a Windows static server - an Amazon Linux static server\n\ndeployed in both PPRD and PROD account You should have two different deployment of this product in each account\n\n:::\n\n:::tip\n\nIf your application installation can be automated : you can consider using AMI which will add more agility to your deployment and allow to tend to immutable infrastructure With this setup, patching is not needed anymore a new version of your application containing all new patches can be easily deployed using tools like Packer\n\n:::\n\nDeployment configuration\n\nYou may define the following configuration options:\n\nCategory Configuration option Details Allowed values Main parameters Name for patching resources Name for patching resources the name will be added on every deployed resources Could be ApplicationName_Environment \u200b Is Patching enabled ? A MANDATORY space separated list of valid Air Liquide e-mail. true or false  Patch Baseline - Operating System Operating system The operating system for the patch baseline -AMAZON-LINUX AMAZON-LINUX-2 -REDHAT-ENTERPRISE-LINUX -WINDOWS \u200b Operating System Patching level Refer to patching level The desired number of days between NON PRODUCTION and PRODUCTION patching The number of days between non production instance and you production instances. This value will be used to find the same baseline the day you patch production. Put 0 for non production  Microsoft - Application Patching ( Only for Windows OS ) Microsoft Product - Family - Application 1 Allow to patch a Microsoft application ( Windows Only ). * Allow to patch all microsoft applications Microsoft Product - Family - Application 2 Allow to patch""]","The key configuration options for customizing patch levels and schedules for operating systems and applications using this solution are:

1. Operating System Patching level: This allows selecting the desired patching level (LEVEL1-Managed, LEVEL2, LEVEL3, LEVEL4, LEVEL5-All) for the operating system.

2. Microsoft Product - Family - Application 1 and 2: These options allow specifying Microsoft applications to be patched (for Windows operating systems only).

3. The desired number of days between NON PRODUCTION and PRODUCTION patching: This option allows setting the number of days between patching non-production and production instances, allowing for validation before applying patches to production.","[{'source': 'cloud-services\\docs\\aws-lab-essential\\07-service-catalog\\11-synced-patching.md', 'filename': 'cloud-services\\docs\\aws-lab-essential\\07-service-catalog\\11-synced-patching.md'}]",multi_context,
12,12,"Per the Auto OS Patching docs, what 'Patch Group' & 'Patch Schedule' tag vals should be set for RHEL envs to get latest OS patches on 1st Sat of each month?","[""## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## Automated OS Patching for Windows & Linux\n\nThis tool provides a simple way to patch automatically your Windows and Linux EC2 instances if you don't have the possibility to frequently replace the base AMI in an immutable infrastructure fashion.\n\nActivating the solution is as simple as applying a tag to your EC2 instances.\n\nNote that SSM will automatically reboot your instances so you must be careful with the application stop/start and the associated outage.\n\nThe Automated OS Patching is available for Windows Server 2016/2019, Amazon Linux 2 and Red Hat Enterprise Linux 7 & 8.2.\n\n:::info\n\nJump directly to the Get Started section or our examples\n\n:::\n\nDescription\n\nThe solution is based on a GIO-managed configuration of System Manager Patch Baselines and Maintenance Windows to patch running instances.\n\nMaintenance Windows define a schedule for when to perform maintenance (e.g. OS Patching) tasks to instances\n\nPatch baselines define which patches are approved for installation on your instances\n\nAs part of the solution, GIO is providing already-configured Maintenance Windows and Patch Baselines that you can choose when using the Automated OS Patching tool.\n\nAll SSM documents and other necessary assets are already deployed in your Essential account so you simply have to designate which instance you want to patch without configuring any complex infrastructure.\n\nScope of patching\n\nThe Automated OS Patching comes with predefined patch categories for each distribution.\n\n:::caution\n\nOnly operating system patches are applied. Middleware and applications are not in this service's scope.\n\n:::\n\nThe details of the patch baselines can be found in the AWS SSM Console > Patch Manager: filter Patch Baselines for Owner: Self\n\nWindows Server 2016 & 2019\n\nOnly the following OS patches are applied to the Windows instances:\n\nClassification Severity Include non-security updates SecurityUpdates Critical only No CriticalUpdates All No\n\nFor more details about the classification and severity for Windows, please check this official Microsoft blog post about Windows Update categories\n\nAmazon Linux 2, RHEL 7 & 8\n\nOnly the following OS patches are applied to the Linux instances:\n\nClassification Severity Include non-security updates Security Critical and Important No Bugfix All No Moderate All No\n\nConcepts\n\nPatch Baselines\n\nThe Patch Baselines are the one of the two core components of GIO OS Patching and are based on the following configurable dimensions:\n\nThe Approval Delay which allows you to define a limitation on patch's release date\n\nThe target Operating System of the instances to patch\n\nThe Patch Baseline is applied to an instance by setting the tag Patch Group indicating values for all the dimensions stated above: [ApprovalDelay]-[OperatingSystem]\n\nApproval Delay\n\nThe Approval Delay allows you to apply only patches which are older than a defined number of days. This is especially useful for multi-environment projects when you patch dev, pprd and then prod.\n\nFor example, let say you want to patch your dev environment on the first Saturday, then the pprd on the second Saturday (7 days after dev) and finally the prod on the fourth Saturday (21 days after dev). What you want is that the exact same patches are applied progressively to your environments for consistency sake and to avoid any bugs.\n\nWithout an Approval delay, pprd would be patched with patches that would have been released between the first and second Saturday, whereas Dev would have been patched with patched available 7 days earlier. Same goes for prod which will have 14 days of patch delta with pprd and 21 days with dev.\n\nIn that case, patch approval delay solves this by letting you choose an approval delay: 21 days for prod and 7 days for pprd will do the trick. This way, you are ensuring that the same patched are applied consistently and progressively across your environments.\n\nApplicable value in Patch Group tag Approval delay Recommendation D0 All patches Development, Sandbox D7 Only patches that are older than 7 days Preproduction, staging, QA D14 Only patches that are older than 14 days Preproduction, staging, QA D21 Only patches that are older than 21 days Production D28 Only patches that are older than 28 days Production\n\nPlease note that only those values are available by default in the solution.\n\nIf you don't deactivate the patching in a D7, D14 or D21 environment, then SSM will automatically apply the patches in due time. For example, if a patching in Dev with D0 has caused an outage, you want to avoid it to go to the next environment. To do so, deactivate the patching by removing""]","For RHEL environments to get the latest OS patches on the first Saturday of each month, the 'Patch Group' tag should be set to 'D0-AmazonLinux2/RHEL' and the 'Patch Schedule' tag should be set to 'PatchtDay=Saturday;PatchWeekOfMonth=1stWeek'","[{'source': 'cloud-services\\docs\\aws-lab-essential\\06-tooling\\03-os-patching.md', 'filename': 'cloud-services\\docs\\aws-lab-essential\\06-tooling\\03-os-patching.md'}]",multi_context,
13,13,"How does the SF utilize GitLab CI/CD, container images, and runners to automate SDLC processes?","['## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## Quick Start\n\nPrerequisites\n\nYou must have access to the Software Factory\'s GitLab. Follow this guide if not.\n\nRecommended GitLab documentation\n\n:::info\n\nThis documentation will help you to get started in the Software Factory environment but is not intended to describe how to use GitLab CI. For non-Software Factory-specific info (GitLab CI features, specs, etc...) please have a look at the official GitLab documentation:\n\n:::\n\nGetting started with GitLab CI/CD: understand how GitLab CI/CD works\n\nGitLab CI/CD configuration file: learn all about the ins and outs of .gitlab-ci.yml\n\nPipelines and jobs: configure your GitLab CI/CD pipelines to build, test, and deploy your application\n\nHow to use the GitLab CI/CD feature in the Software Factory?\n\nCreate a pipeline configuration file\n\nThe GitLab CI/CD feature is enabled by default for all projects. To create a pipeline and execute jobs you simply have to create a pipeline configuration file, named .gitlab-ci.yml in the root folder of your project.\n\nHere is a very simple example of a job scanning CloudFormation templates for security purposes:\n\nChoose a runner\n\nTo use a shared runner, it is not necessary to indicate a tag. Any job without tag will be picked by a shared runner.\n\nTo use a Dedicated Runner, you must set a tag value specifying the tag name that has been given to you by GIO. This runner must be enabled for your project or the group in which your project resides.\n\nSpecify a GIO-managed container image\n\nYou can bring your own image, yet, we do recommend that you always use a GIO-provided base image to execute your job or to create a new container image that suits your needs. That way, you can benefit from all the setup already embedded in GIO base images such as certificates, tools properly configured, etc.\n\nTrigger a pipeline\n\nGitLab will automatically execute the jobs when a .gitlab-ci.yml file is present in the project at any push. Check the pipeline execution in the GitLab interface under ""Pipelines"" in the navigation side-panel of your project. You can also leverage Pipeline Schedules to schedule jobs.\n\nCI/CD snippets\n\nUse the provided samples to get started with CI/CD jobs in GitLab CI. Add the .gitlab-ci.yml file at the root of your project, commit, push and check the pipeline execution in the GitLab interface.\n\n:::info\n\nSee our snippets in our shared repository\n\n:::']","The Software Factory (SF) utilizes GitLab CI/CD in the following ways:

1. Projects in the SF have GitLab CI/CD enabled by default, allowing the creation of pipeline configuration files (.gitlab-ci.yml) to define jobs and automate processes.

2. The SF provides shared runners that can execute jobs without needing to specify a tag. Dedicated runners with specific tags can also be used for projects or groups.

3. The SF recommends using GIO-provided base container images to execute jobs or create new container images. These base images come pre-configured with certificates, tools, and other setups.

4. Pipelines are automatically triggered when a .gitlab-ci.yml file is present in the project and a commit is pushed. Pipeline schedules can also be leveraged to schedule jobs.

5. The SF provides CI/CD snippets and samples in a shared repository to help users get started with defining CI/CD jobs in the .gitlab-ci.yml file.

In summary, the SF leverages GitLab CI/CD, container images, and runners to automate software development life cycle processes by enabling users to define pipelines, jobs, and utilize pre-configured resources within the SF environment.","[{'source': 'cloud-services\\docs\\sf\\04-gitlab-ci\\01-quick-start.md', 'filename': 'cloud-services\\docs\\sf\\04-gitlab-ci\\01-quick-start.md'}]",multi_context,
14,14,What services and capabilities are offered with a GCP Self-supporting Project?,"['## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## Get Started with your GCP Essential Project\n\nPrerequisites\n\nBefore starting to read this section, we assume that you have already requested a GCP Essential project through an Open Demand\n\n:::tip To be part of this journey, your project must be compliant with Air Liquide Cloud Computing Policy\n\nNote that GCP projects require standard validation from the TADA* committee. However, BigQuery projects enjoy automated and streamlined validation.\n\nTechnology and Architecture Design Authority :::\n\nServices offering available\n\nGCP Self-supporting Project:\n\nYou will not be connected to Air Liquid networks but you will be able to use all GCP self-supporting feature like BigQuery. You will be able to interact with any system with service account for instance.\n\nThe permission on your project will fit with your context and the least privilege principle will be applied.\n\nAir Liquide Network Connected Project:\n\nA standard Shared VPC configuration, designed and built by GIO in the organization:\n\nConnected to Air Liquide networks (on-premise, AWS, etc.) through VPNs.\n\nCIDR compatible with Air Liquide internal network.\n\nPredefined subnets deployed to enable high-availability and secure architecture in 3 hubs (Europe, Apac, Americas).\n\nVPC Flow logs activated.\n\nDefault Firewall rule applied and custom Secure tags can be used depending of your context.\n\nIts configuration is operated by GIO but you can deploy - almost, what you want inside.\n\nAll internet inbound flows are filtered through a Web Application Firewall.\n\n:::caution It is not possible to change a GCP Self-supporting project into an AL Network connected project. :::\n\nConnect to the GCP Console\n\nGCP is using Google Workspace identities as Identity directory :\n\nYou will access GCP projects using Kite users or groups.\n\nIn a near future, GCP will be integrated with Okta when Google Workspace authentication will be migrated from Multipass to Okta.\n\nGIO sets up Standard IAM Roles and Policies into your project for security, administration and governance purposes.\n\nStandard configuration includes:\n\nDefault permissions managed by GIO.\n\nRoles for project administration and interfaces with external systems (e.g. billing, securityâ¦).\n\nRole with the principle of least privilege will be applied by GIO. It involves assigning the minimum set of permissions required for users to carry out your job responsibilities effectively, while preventing you from accessing or modifying resources that are not relevant for your job.\n\n:::tip Use this link to access Google Cloud Console: https://console.cloud.google.com :::\n\nDeploy infrastructure with IaC\n\nWe strongly recommend to use Terraform to deploy and manage your infrastructure in GCP. Never deploy or modify your infrastructure via the console to ensure consistency, repeatability and standardization of your infrastructure. It also possible to use the Google service Deployment Manager to manage you resources.\n\nPolicies\n\nOur Landing zone contains non-negotiable baseline policies that you canât modify, whose purpose is to protect the assets deployed into your project by GIO to secure it. This constraints are only serving as guardrails and should be transparent to you.\n\nSupport\n\nIf you have an incident with network connectivity, a landing zone asset or if you are struggling with your project permissions, you can contact the cloud services team at wwgio-gcp-admins@airliquide.com.\n\nFor all other matters, please raise a GCP support ticket. Please refer to the support documentation.']","With a GCP Self-supporting Project, you will be able to use all GCP self-supporting features like BigQuery. You will be able to interact with any system with service accounts, but you will not be connected to Air Liquide networks.","[{'source': 'cloud-services\\docs\\gcp-essential\\02-quick-start\\01-gcp-essential.md', 'filename': 'cloud-services\\docs\\gcp-essential\\02-quick-start\\01-gcp-essential.md'}]",simple,
15,15,How do you start an execution in the AWS Step Functions console to trigger the GP3 EBS migration process?,"[""\n\nTrigger GP3 EBS Migration\n\r\nWhen you are ready to start the operation:\r\n\r\n\r\n\r\n\r\nOpen to the AWS Step Functions console using the link from the product's outputs called `stateMachineURL`.\r\n\r\n\r\n\r\n!Product Outputs  \r\n\r\n\r\n\r\n\r\nInto Step Functions console, click on **Start execution**\r\n\r\n\r\n\r\n!Start execution 1\r\n\r\n\r\n\r\n\r\nLeave the default Input (it is ignored) and click again on **Start execution**\r\n\r\n\r\n\r\n!Start execution 2\r\n\r\n\r\n\r\n\r\nYou can follow the execution into the Step Functions console\r\n\r\n\r\n\r\n!Follow execution\r\n\r\n\r\n\r\n\r\n""]","To start an execution in the AWS Step Functions console to trigger the GP3 EBS migration process, follow these steps: 1) Open the AWS Step Functions console using the link from the product's outputs called 'stateMachineURL'. 2) Click on 'Start execution'. 3) Leave the default Input (it is ignored) and click 'Start execution' again. 4) You can then follow the execution in the Step Functions console.","[{'file_path': 'c:\\Users\\soel.megdoud\\Documents\\RAG Air Liquide\\rag_cs\\cloud-services\\docs\\aws-lab-essential\\07-service-catalog\\04-gp3_migration.md', 'file_name': '04-gp3_migration.md', 'file_size': 12238, 'creation_date': '2024-07-31', 'last_modified_date': '2024-08-20'}]",simple,
16,16,What error code is associated with an invalid domain during the provisioning process?,"['                                        |\r\n|        AD        |   `60`   | Failed to join domain: SPN value must be unique forest-wide | Ensure that computer name or provided SPN is unique in the forest                                                                                       |\r\n|        AD        |   `80`   | Invalid group ID, validate regex `0-9A-Z]{2}`           | Check value of [`gio:provision:domain:groupId` tag                                   |\r\n|        AD        |   `81`   | Invalid domain                                           | Check value of `gio:provision:domain:name` tag                                         |\r\n|        AD        |   `82`   | Invalid computer SPN                                     | Check value of `gio:provision:domain:computerSpn` tag                           |\r\n|       SEP        |  `110`   | Antivirus configuration error                            | Contact GIO Cloud Engineering                                                                                         |\r\n\r\n']",The error code associated with an invalid domain during the provisioning process is 81.,"[{'file_path': 'c:\\Users\\soel.megdoud\\Documents\\RAG Air Liquide\\rag_cs\\cloud-services\\docs\\aws-lab-essential\\08-tutorials\\03-windows-workloads\\01-tuto-windows-ec2.md', 'file_name': '01-tuto-windows-ec2.md', 'file_size': 29385, 'creation_date': '2024-07-31', 'last_modified_date': '2024-08-20'}]",simple,
17,17,Why might admin credentials be required when installing Git on an Air Liquide PC?,"['\n\nInstall git\n\r\nLaunch the installation executable that you\'ve downloaded and follow the wizard. Leave the default values or customize if you are conversant enough with git advanced configuration.\r\n\r\nOn an Air Liquide PC with Windows, Git should be eventually deployed under `C:\\Users\\%UserName%\\AppData\\Local\\Programs\\Git\\`\r\n\r\n:::info\r\n\r\nOn an Air Liquide device, you might be prompted for admin credentials.  Though this is unlikely to happen, you can still install Git by being granted temporary elevated privileges by using YourWay.\r\nTo request YourWay, browse to MyStore Portal (aka TomPortal), look up ""YourWay - Cyberark EPM Agent (GIO)"" and order it. Requires your manager approval.  \r\nOnce installed on your PC, you should be prompted by YourWay when retrying the Git installation. Provide a justification and submit.\r\n\r\n:::\r\n\r\n']",Admin credentials might be required when installing Git on an Air Liquide PC because Git is being deployed in a system directory (C:\Users\%UserName%\AppData\Local\Programs\Git\) which typically requires administrative privileges for installation.,"[{'file_path': 'c:\\Users\\soel.megdoud\\Documents\\RAG Air Liquide\\rag_cs\\cloud-services\\docs\\sf\\03-gitlab\\01-quick-start.md', 'file_name': '01-quick-start.md', 'file_size': 6280, 'creation_date': '2024-07-31', 'last_modified_date': '2024-08-20'}]",simple,
18,18,What types of email notifications are sent as part of the automation process for managing IAM User access keys in AWS?,"['IAM User Access Key Management\n\nOverview\n\nThe purpose of this automation is to avoid to have IAM User access keys remain active when it is not used or when it is expired. It is the operational enforcement of Air Liquide Cloud security policies.\n\nIAM User access keys are considered as inactive when showing no activity in the past 40 days.\n\nIAM User access keys are considered as expired 365 days after creation (ie. IAM User access keys must be rotated before they reach 365 days of age.)\n\nArchitecture\n\nAutomation sequence\n\nScope\n\nThis automation is executed: - into all Essential and Managed accounts - for all existing IAM Users\n\nAre excluded from the automation: - Lab Accounts - The AWS environments managed by Claranet for the HDS scope in Europe\n\nAre deactivated: - Keys not used from at least 40 days. - Keys created from at least 365 days.\n\nA deactivated key still appears in the IAM User Access Keys list but is not usable.\n\nAre deleted: - For expired keys: Keys deactivated 31 days ago, even if keys is reactivated in the meantime. So when the age of the key is 396 days old. - For non used keys: Keys deactivated 31 days ago except if key is reactivated and used again. So if key is not used for 71 days.\n\nSchedule\n\nThis automation is scheduled once a day, between 12h-13h UTC globally.\n\nHow it works\n\nAutomation proceeds as follows: - List all IAM User access keys deployed in the AWS account - For each key in the list: - Check for how many days key is not used - Check for how many days key was created - If key is not used for almost 40 days: send a warning email to owner of key and owner of AWS account - If key is not used for more than 40 days: deactivate the key and send an informational email to owner of key and owner of AWS account - If key is created for almost 365 days: send a warning email to owner of key and owner of AWS account - If key is created for more that 365 days: deactivate the key and send an informational email to owner of key and owner of AWS account (even if the key is used) - If key was deactivated for the first time 31 days ago: delete it\n\n:::tip\n\nThe automation will use the AWS API to deactivate or delete keys. If you have deployed it via CloudFormation, Terraform, etc. you may have a drift from your corresponding stack.\n\n:::\n\nHow to\n\nHow to configure the recipients of the warning emails\n\nTo be informed about events regarding your IAM User Access Keys (near expiration, deactivation, etc...), you can specify the recipients of the warning emails. Use the following available tags:\n\nTag Key Value gio:iamUser:owner A space separated list of up to 5 IAM User owner e-mail addresses â To recipients of mails. gio:iamUser:cc A space separated list of up to 5 IAM User owner e-mail addresses â Cc recipients of mails.\n\n:::caution\n\nThe tags are case sensitive\n\n:::\n\n:::info\n\nYou can find more detailed information regarding IAM Users here.\n\n:::\n\nHow to deploy a compliant IAM User\n\nYou may use the self-service solution ""IAM Users"" which deploys a simple IAM User compliant with cloud and Air Liquide security standards.\n\nGet Started with IAM User deployment\n\nFAQ\n\nWhy do we deactivate expired keys? An IAM User access key is a secret credential that can be used to access internal Air Liquide IT resources. The older this credential is, the more we have a risk of having it stolen/compromised. This is why we have an Air Liquide security golden rule regarding the age of those keys and the obligation to change/rotate it at least once a year.\n\nHow can I prevent this deactivation? By respecting the Air Liquide security policy, so changing/rotating your keys once a year no deactivation will occur as your keys will not reach the maximum time life of 1 year.\n\nWhat is the impact of this automatic deactivation? After deactivation of IAM User access key, the key is no longer usable. If you have a workload which uses the key, this workload will stop working just after deactivation.\n\nWill I receive a notification? If you are the AWS account owner, or if you are the IAM User owner, you will receive a mail to inform you about the deactivation with the reference of the deactivated key.\n\nI own an IAM User. Why am I not receiving any mail? It means the tags on IAM Users are not properly set up with your email address.']","The following email notifications are sent as part of the automation process for managing IAM User access keys:

1. Warning email sent to the owner of the key and the owner of the AWS account when a key is not used for almost 40 days.

2. Informational email sent to the owner of the key and the owner of the AWS account when a key is deactivated after not being used for more than 40 days.

3. Warning email sent to the owner of the key and the owner of the AWS account when a key is created for almost 365 days.

4. Informational email sent to the owner of the key and the owner of the AWS account when a key is deactivated after being created for more than 365 days, even if the key is used.","[{'source': 'cloud-services\\docs\\aws-lab-essential\\04-landing-zone-references\\07-autom-ref\\01-iam-user-key.md', 'filename': 'cloud-services\\docs\\aws-lab-essential\\04-landing-zone-references\\07-autom-ref\\01-iam-user-key.md'}]",simple,
19,19,"What are the key details provided about the ExpressRoute circuit configuration for Air Liquide's network connectivity in Europe, including the circuit name, location, bandwidth, and peering details?","['Core Platform\n\nThis documentation helps to understand how Azure is designed and implemented at Air Liquide. For now the Azure Platform is used in 2 specific contexts:\n\nBIG\n\nDynamics 365\n\nNaming convention\n\nFor Azure resources naming, please refer to the naming convention documentation.\n\nAzure Locations\n\nCurrently, Azure infrastructure deployment is restricted to 7 regions:\n\nNorth Europe(northeurope)\n\nWest Europe(westeurope)\n\nUS East 2(eastus2)\n\nSoutheast Asia(southeastasia)\n\nAustralia East(australiaeast)\n\nFrance Central(francecentral)\n\nCanada Central(canadacentratl)\n\nThis means that each of these locations has an on-site Microsoft(or Microsoft sponsored) data-center that be can use to deploy your resources. There are multiple factors you should consider to select the appropriate region for a given workload:\n\nCompliance(data hosting strategy)\n\nFinancial(resource prices)\n\nTechnical(resource availability)\n\n:::caution\n\nPlease take the time to review each of those factors before deploying to an environment.\n\n:::\n\nCost Management + Billing\n\nIn order to manage Air Liquide Azure billing and effective cost, schedules are configured to export billing information of all Azure subscriptions.\n\nExport\n\nTwo daily exports are scheduled:\n\nDestination Export Frequency Storage account Dig@GIO csopscontrol Daily alneubigdevsadig01 Actu@GDO 0061DailyExportMonth2Date Daily alneubigprodadlgdo01\n\nManagement groups\n\nUsing Management groups helps to manage access, policy, and compliance by grouping multiple subscriptions together.\n\nHere is the distribution of all Air Liquide Azure subscriptions across Management Groups:\n\nAzure Core Policy\n\nPolicies\n\nAzure Policy is a service in Azure that you use to create, assign, and manage policies. These policies enforce various rules and effects over your resources, so those resources stay compliant with our corporate standards and service level agreements. Azure Policy meets this need by evaluating your resources for non-compliance with assigned policies. All data stored by Azure Policy is encrypted at rest. For example, we can have a policy to allow only a certain SKU size of virtual machines in your environment. Once this policy is implemented, new and existing resources are evaluated for compliance. With the right type of policy, existing resources can be brought into compliance.\n\nThe Compliance Overview shows you the different level of compliancy with each policy linked to a specific subscription.\n\nA Policy should be a Definition, assigned on a specific resource or part of a specific Initiative. Each Policy definition match a specific need.\n\nExample:\n\nPolicy Definition ""Deny SQL Databases Editions"" deny the deployment of specific database edition as Hyperscale. Policy looks like:\n\nAll the microsoft documentation concerning Azure Policy is defined here.\n\nInitiatives\n\nAn initiative definition is a collection of policy definitions that are tailored towards achieving a singular overarching goal. Initiative definitions simplify managing and assigning policy definitions, they simplify by grouping a set of policies as one single item. For example, we could create an initiative titled Enable Monitoring in Azure Security Center, with a goal to monitor all the available security recommendations in Azure Security Center.\n\nYou\'ll find Initiatives in the Definition Tab:\n\nAzure Core Initiative\n\nCategory : App Services\n\nDeny if HTTPS is not used in your App Services\n\nDeny if Latest TLS version is not used in your App Services\n\nCategory : SQL\n\nDeny If SQL Databases Hyperscale and Business Critical editions\n\nAn Azure Active Directory administrator should be provisioned for SQL servers\n\nTransparent Data Encryption on SQL databases should be enabled\n\nAudit Azure SQL Database should have the minimal TLS version of 1.2\n\nDeny if Latest TLS version is not used in your SQL Server\n\nCategory: Storage\n\nDeny if Storage Account is using allowBlobPublicAccess\n\nDeny if Storage Account is not using HTTPS traffic\n\nDeny if Storage Account is not using TLS 1.2\n\nCategory : Location\n\nAllowed locations : North Europe, West Europe, US East 2, Southeast Asia, Australia East, Canada Central, France Central\n\nAzure BIG Initiative\n\nCategory: Tags\n\nInherit ProjectName tag from the resource group\n\nInherit ProjectShort tag from the resource group\n\nInherit CustomerName tag from the resource group\n\nInherit EnvironmentName tag from the resource group\n\nInherit CostCenter tag from the resource group\n\nInherit InitiativeCode tag from the resource group\n\nInherit Confidentiality tag from the resource group\n\nInherit BuildVersion tag from the resource group\n\nCategory:', "" Location\n\nAllowed locations : North Europe, US East 2, Southeast Asia, Australia East\n\nCategory: Storage\n\nAudit Prod Storage Account Sku\n\nAudit Non-Prod Storage Account Sku\n\nAzure DYN Initiative\n\nCategory: Tags\n\nInherit ProjectName tag from the subscription\n\nInherit ProjectShort tag from the subscription\n\nInherit CustomerName tag from the subscription\n\nInherit EnvironmentName tag from the subscription\n\nInherit CostCenter tag from the subscription\n\nCategory: Location\n\nAllowed locations : Canada Central, France Central, North Europe\n\nCategory: Virtual Machine\n\nMicrosoft IaaSAntimalware extension should be deployed on all servers\n\nCategory: Virtual Network\n\nInternet-facing virtual machines should be protected with network security groups\n\nNon-internet-facing virtual machines should be protected with network security groups\n\nInitiatives and Managements groups\n\nAzure CORE Group Management is propagating the Azure Core Initiative\n\nAzure BIG Group Management is propagating the Azure BIG Initiative\n\nAzure DYN Group Management is propagating the Azure DYN Initiative\n\nAzure Core Reporting\n\nReporting data from Azure platform helps us to monitor infrastructure state. Configuration compliance could be checked using operational data.\n\nExpressRoute\n\nEurope\n\nExpressRoute lets Air Liquide extend its on-premises networks into the Microsoft cloud over a private connection with dedicated bandwidth from a connectivity provider. In the Air liquide context, ExpressRoute is mostly used to reach Microsoft public services such as Azure AD, Azure SQL Database, Azure Storage, etc. Nevertheless, an Azure Private peering is available to establish a connection with private networks on Azure if it is necessary in future scenarios. A virtual cross-connection through a connectivity provider at a co-location facility offers more reliability, faster speeds, consistent latencies, and higher security than typical connections over the Internet to Air Liquide.\n\nThe European Expressroute circuit is hosted on the subscription GIO-CORE_SERVICES.\n\nAll the resources related to the European ExpressRoute circuit are deployed in the North Europe region:\n\nResource group: al-neu-core-prod-rgp-expressroute\n\nExpressRoute circuit: al-neu-core-prod-ercircuit-london\n\nProvider: Level 3 Communications - Exchange\n\nPeering Location: London\n\nBandwidth: 200 Mbps\n\nSKU: Standard\n\nBilling model: Metered\n\nTwo peerings are active on this ExpressRoute circuit:\n\nAzure Private:\n\nPrimary subnet: 169.254.254.180/30\n\nSecondary subnet: 169.254.254.176/30\n\nVLAN ID: 176\n\nConnection: al-neu-core-prod-cn-alnetwork-to-vnet-hub-vngtw-01\n\nVirtual Network: al-neu-core-prod-vnet-hub\n\nVirtual Network Gateway: al-neu-core-prod-vnet-hub-vngtw-01\n\nSKU: Standard\n\nGateway type: ExpressRoute\n\nPublic IP: 40.69.26.54\n\nMicrosoft:\n\nPrimary subnet: 185.188.176.40/30\n\nSecondary subnet: 185.188.176.44/30\n\nIpv4 Advertised public prefixes: 185.188.176.0/22\n\nVLAN ID: 40\n\nRoute filters\n\nA route filter lets us identify services we want to consume through our ExpressRoute circuits' Microsoft peering. It is essentially a list of all allowed BGP community values. Once a route filter resource gets defined and attached to an ExpressRoute circuit, all prefixes that map to the BGP community values are advertised to Air Liquide's network.\n\nRoute filter name: al-neu-core-prod-ercircuit-london-rf\n\nRule name: al-neu-core-prod-ercircuit-london-rf-rule-01\n\nAllowed service communities:\n\nName Value Azure North Europe 12076:51003 Azure Global Services 12076:5050 Azure West Europe 12076:51002 Azure Active Directory 12076:5060\n\nAir Liquide Network Architecture\n\n:::info\n\nWaiting for documentation from Network Team\n\n:::\n\nContacts\n\nIf there is any issue with the European ExpressRoute configuration, besides the Azure Infra Team, it is possible to reach GIO Network Team, specifically Karim Rahmani(karim.rahmani-sc@airliquide.com).\n\nAsia\n\nThe Asian Expressroute circuit is hosted on the subscription""]","The key details provided about the ExpressRoute circuit configuration for Air Liquide's network connectivity in Europe are:

- ExpressRoute circuit name: al-neu-core-prod-ercircuit-london
- Location: London
- Bandwidth: 200 Mbps
- SKU: Standard
- Billing model: Metered
- Two peerings are active:
  - Azure Private peering with primary subnet 169.254.254.180/30 and secondary subnet 169.254.254.176/30, VLAN ID 176
  - Microsoft peering with primary subnet 185.188.176.40/30, secondary subnet 185.188.176.44/30, advertised public prefixes 185.188.176.0/22, VLAN ID 40
- Route filter name: al-neu-core-prod-ercircuit-london-rf
- Allowed service communities in the route filter: Azure North Europe, Azure Global Services, Azure West Europe, Azure Active Directory","[{'source': 'cloud-services\\docs\\azure-core-platform\\01-core-platform.md', 'filename': 'cloud-services\\docs\\azure-core-platform\\01-core-platform.md'}, {'source': 'cloud-services\\docs\\azure-core-platform\\01-core-platform.md', 'filename': 'cloud-services\\docs\\azure-core-platform\\01-core-platform.md'}]",simple,
20,20,Any tut on GitLab Groups' struct for rights/perms delegation?,"['\n\nAuthorizations in GitLab\n\r\nTo delegate rights and permissions balancing security and client-autonomy, GIO has implemented a specific Groups structure in GitLab. You need to understand how it has been created and how it works before using the GitLab as a service. Developers as well as managers and project managers should read this tutorial.\r\n\r\n']","Yes, the context provides an overview of the GitLab Groups structure implemented by GIO for delegating rights and permissions while balancing security and client autonomy. It mentions that developers, managers, and project managers should read this tutorial to understand how the Groups structure has been created and how it works before using GitLab as a service.","[{'file_path': 'c:\\Users\\soel.megdoud\\Documents\\RAG Air Liquide\\rag_cs\\cloud-services\\docs\\sf\\03-gitlab\\04-user-access.md', 'file_name': '04-user-access.md', 'file_size': 3640, 'creation_date': '2024-07-31', 'last_modified_date': '2024-08-20'}]",reasoning,
21,21,Which svc's avail. impacts LZI deploy. during regional outage in main hub?,"["" Impact No Impact No Impact N/A The LZI CLI Server is used into the Cockpit only for deployment not for run. So no impact on run. \u200b Deploy No Impact Impacted Impact into region EC2 with ELB CloudFormation It is impacted during a regional issue on main LZI region. Furthermore this service is not impacted by an AZ issue.\n\nStorage\n\nStorage in Landing Zone are mainly global Shared S3 buckets and buckets into each account, some pre-configure S3 buckets.\n\n\u200bComponent Function AZ Trouble/Lost Main HUB region Trouble/Lost Other region Trouble/Lost Services Used Comment Global S3 Shared Run No Impact No Impact No Impact S3 As we deploy a copy of shared data into an S3 bucket into each regions, it remain always a bucket available. \u200b Deploy No Impact Impacted Impact into region S3 New data are deployed by upload data into a main shared bucket into Main HUB region. Pre-configure buckets per account Run No Impact Impact into region Impact into region S3 Interaction with S3 buckets into impacted region may not work. \u200b Deploy No Impact Impacted Impact into region CloudFormation Component deployed using cloudformation stackset from the Cockpit account. See Cockpit section for more information.\n\nCompute\n\nHere you can find all LZI components related to compute services, like EC2 and Lambda.\n\n\u200bComponent Function AZ Trouble/Lost Main HUB region Trouble/Lost Other region Trouble/Lost Services Used Comment EC2 Backup Run No Impact Impact into region Impact into region EC2 Snapshot Lambda Standard EC2 backup provided by LZI is based on EC2 snapshot and Lambda which is impacted only with a global region outage and only into the concerned region. \u200b Deploy No Impact Impacted Impact into region CloudFormation Component deployed using cloudformation stackset from the Cockpit account. See Cockpit section for more information. EC2 Patching Run No Impact Impact into region Impact into region SSM Patching may not work into impacted region during the outage. \u200b Deploy No Impact Impacted Impact into region CloudFormation Component deployed using cloudformation stackset from the Cockpit account. See Cockpit section for more information. Lambda (layers, cfn macros, ...) Run No Impact Impact into region Impact into region Lambda Lambda may be unavailable during regional outage. \u200b Deploy No Impact Impacted Impact into region CloudFormation Component deployed using cloudformation stackset from the Cockpit account. See Cockpit section for more information. SSM Parameter store Run No Impact Impact into region Impact into region SSM See SSM resiliency documentation. \u200b Deploy No Impact Impacted Impact into region CloudFormation Component deployed using cloudformation stackset from the Cockpit account. See Cockpit section for more information. Golden AMI Run No Impact Impact into region Impact into region EC2 AMI In case of regional outage, EC2 deployment from Golden AMI may be not work. \u200b Deploy No Impact Impacted Impact into region Software Factory CloudFormation Packer Lambda SSM Step Function Golden AMI are build into HUB Main region, so an outage on this region globally impact AMI creation.\n\nNetwork\n\nHere you can find all LZI components related to network connectivity.\n\n\u200bComponent Function AZ Trouble/Lost Main HUB region Trouble/Lost Other region Trouble/Lost Services Used Comment VPC Run Impact subnets in AZ Impact into region Impact into region VPC During an AZ outage, all subnets into this AZ can be impacted, but other components of VPC continue to be operational. Duing a global region outage, you may have a complete lost of network connectivity for your VPC. Note that with LZI, it is mandatory to deploy VPCs using at least 2 AZs. \u200b Deploy No Impact Impact into region Impact into region CloudFormation Component deployed using cloudformation into the same region than VPC. So only deployment into impacted region my not work. Transit Gateway Run No Impact Impact into region Impact into region TGW TGW is a regional service not impacted by an AZ outage. But in case of regional outage, network connectivity may not work globally into the region. \u200b Deploy No Impact Impact into region Impact into region CloudFormation Component deployed using cloudformation into the same region than TGW. So only deployment into impacted region my not work.\n\nSecurity\n\n\u200bComponent Function AZ Trouble/Lost Main HUB region Trouble/Lost Other region Trouble/Lost Services Used Comment Cloudtrail Run No Impact Impact into region Impact into region Cloudtrail Check Cloudtrail resiliency documentation. \u200b Deploy No Impact Impacted No Impact CloudFormation Component deployed using cloudformation stackset from the Cockpit account. See Cockpit section for more information. Note that deployment is done only into HUB Main region. This is why, for Other region Trouble, we don't have any impact for deployment Function. Config Run No Impact Impact into region Impact into region Config Check AWS Config resili""]","During a regional outage in the main hub region, the deployment of several LZI components would be impacted, including EC2 Backup, EC2 Patching, Lambda functions, SSM Parameter Store, Golden AMI, VPC, Transit Gateway, and CloudTrail. These components are deployed using CloudFormation StackSets from the Cockpit account in the main hub region, so their deployment would be impacted by a regional outage in that region.","[{'source': 'cloud-services\\docs\\aws-lab-essential\\03-architecture-and-concepts\\11-lzi-res.md', 'filename': 'cloud-services\\docs\\aws-lab-essential\\03-architecture-and-concepts\\11-lzi-res.md'}]",reasoning,
22,22,What format enables DB migration from RDS SQL Server to S3?,"['\n\nNative Backup details for SQL Server\n\r\nTo backup a database from SQL Server RDS to a .bak file, youâll do it directly on SSMS with an Amazon provided stored procedure. This SP is named msdb.dbo.rds_backup_database. Itâs included by Amazon for use in every RDS SQL Server instance.\r\n\r\n', '\n\nNative Restore details for SQL Server\nTo restore a database from S3 on an SQL Server RDS instance, youâll use an Amazon provided stored procedure. This SP is named msdb.dbo.rds_restore_database. Itâs also included by Amazon for use in every RDS SQL Server instance.\r\n\r\n']","The .bak file format enables database migration from RDS SQL Server to S3, according to the given context.","[{'file_path': 'c:\\Users\\soel.megdoud\\Documents\\RAG Air Liquide\\rag_cs\\cloud-services\\docs\\aws-managed\\04-aws-managed-operations\\02-database.md', 'file_name': '02-database.md', 'file_size': 12408, 'creation_date': '2024-07-31', 'last_modified_date': '2024-08-20'}, {'file_path': 'c:\\Users\\soel.megdoud\\Documents\\RAG Air Liquide\\rag_cs\\cloud-services\\docs\\aws-managed\\04-aws-managed-operations\\02-database.md', 'file_name': '02-database.md', 'file_size': 12408, 'creation_date': '2024-07-31', 'last_modified_date': '2024-08-20'}]",multi_context,
23,23,What key lifecycle events trigger notifications & actions in the IAM User key automation?,"['IAM User Access Key Management\n\nOverview\n\nThe purpose of this automation is to avoid to have IAM User access keys remain active when it is not used or when it is expired. It is the operational enforcement of Air Liquide Cloud security policies.\n\nIAM User access keys are considered as inactive when showing no activity in the past 40 days.\n\nIAM User access keys are considered as expired 365 days after creation (ie. IAM User access keys must be rotated before they reach 365 days of age.)\n\nArchitecture\n\nAutomation sequence\n\nScope\n\nThis automation is executed: - into all Essential and Managed accounts - for all existing IAM Users\n\nAre excluded from the automation: - Lab Accounts - The AWS environments managed by Claranet for the HDS scope in Europe\n\nAre deactivated: - Keys not used from at least 40 days. - Keys created from at least 365 days.\n\nA deactivated key still appears in the IAM User Access Keys list but is not usable.\n\nAre deleted: - For expired keys: Keys deactivated 31 days ago, even if keys is reactivated in the meantime. So when the age of the key is 396 days old. - For non used keys: Keys deactivated 31 days ago except if key is reactivated and used again. So if key is not used for 71 days.\n\nSchedule\n\nThis automation is scheduled once a day, between 12h-13h UTC globally.\n\nHow it works\n\nAutomation proceeds as follows: - List all IAM User access keys deployed in the AWS account - For each key in the list: - Check for how many days key is not used - Check for how many days key was created - If key is not used for almost 40 days: send a warning email to owner of key and owner of AWS account - If key is not used for more than 40 days: deactivate the key and send an informational email to owner of key and owner of AWS account - If key is created for almost 365 days: send a warning email to owner of key and owner of AWS account - If key is created for more that 365 days: deactivate the key and send an informational email to owner of key and owner of AWS account (even if the key is used) - If key was deactivated for the first time 31 days ago: delete it\n\n:::tip\n\nThe automation will use the AWS API to deactivate or delete keys. If you have deployed it via CloudFormation, Terraform, etc. you may have a drift from your corresponding stack.\n\n:::\n\nHow to\n\nHow to configure the recipients of the warning emails\n\nTo be informed about events regarding your IAM User Access Keys (near expiration, deactivation, etc...), you can specify the recipients of the warning emails. Use the following available tags:\n\nTag Key Value gio:iamUser:owner A space separated list of up to 5 IAM User owner e-mail addresses â To recipients of mails. gio:iamUser:cc A space separated list of up to 5 IAM User owner e-mail addresses â Cc recipients of mails.\n\n:::caution\n\nThe tags are case sensitive\n\n:::\n\n:::info\n\nYou can find more detailed information regarding IAM Users here.\n\n:::\n\nHow to deploy a compliant IAM User\n\nYou may use the self-service solution ""IAM Users"" which deploys a simple IAM User compliant with cloud and Air Liquide security standards.\n\nGet Started with IAM User deployment\n\nFAQ\n\nWhy do we deactivate expired keys? An IAM User access key is a secret credential that can be used to access internal Air Liquide IT resources. The older this credential is, the more we have a risk of having it stolen/compromised. This is why we have an Air Liquide security golden rule regarding the age of those keys and the obligation to change/rotate it at least once a year.\n\nHow can I prevent this deactivation? By respecting the Air Liquide security policy, so changing/rotating your keys once a year no deactivation will occur as your keys will not reach the maximum time life of 1 year.\n\nWhat is the impact of this automatic deactivation? After deactivation of IAM User access key, the key is no longer usable. If you have a workload which uses the key, this workload will stop working just after deactivation.\n\nWill I receive a notification? If you are the AWS account owner, or if you are the IAM User owner, you will receive a mail to inform you about the deactivation with the reference of the deactivated key.\n\nI own an IAM User. Why am I not receiving any mail? It means the tags on IAM Users are not properly set up with your email address.', '\n\nHow it works\n\r\nAutomation proceeds as follows:\r\n- List all IAM User access keys deployed in the AWS account\r\n- For each key in the list:\r\n  - Check for how many days key is not used\r\n  - Check for how many days key was created\r\n  - If key is not used for almost 40 days: send a warning email to owner of key and owner of AWS account\r\n  - If key is not used for more than 40 days: deactivate the key and send an informational email to owner of key and owner of AWS account\r\n  - If key is created for almost 365 days: send a warning email to owner of key and owner of AWS account\r\n  - If key is created for more that 365 days: deactivate the key and send an informational email to owner of key and owner of AWS account (**even if the key is used**)\r\n  - If key was deactivated for the first time 31 days ago: delete it\r\n\r\n:::tip\r\n\r\nThe automation will use the AWS API to deactivate or delete keys. If you have deployed it via CloudFormation, Terraform, etc. you may have a drift from your corresponding stack.\r\n\r\n:::\r\n\r\n']","The key lifecycle events that trigger notifications and actions in the IAM User key automation are: 1) Key not used for almost 40 days (warning email sent), 2) Key not used for more than 40 days (key deactivated and informational email sent), 3) Key created for almost 365 days (warning email sent), 4) Key created for more than 365 days (key deactivated and informational email sent, even if used), 5) Key deactivated for 31 days (key deleted).","[{'source': 'cloud-services\\docs\\aws-lab-essential\\04-landing-zone-references\\07-autom-ref\\01-iam-user-key.md', 'filename': 'cloud-services\\docs\\aws-lab-essential\\04-landing-zone-references\\07-autom-ref\\01-iam-user-key.md'}, {'file_path': 'c:\\Users\\soel.megdoud\\Documents\\RAG Air Liquide\\rag_cs\\cloud-services\\docs\\aws-lab-essential\\04-landing-zone-references\\07-autom-ref\\01-iam-user-key.md', 'file_name': '01-iam-user-key.md', 'file_size': 5074, 'creation_date': '2024-07-31', 'last_modified_date': '2024-08-20'}]",multi_context,
24,0,What is the significance of using MMS DEV and MMS PROD environments when triggering a pipeline?,"[""Change creator v1.6\n\nimport VersionDropDown from '@site/src/components/VersionDropDown'; import VersionData from './version.json';\n\nHow to start\n\nRequest your NinGines GitLab project following the Quick start.\n\nRead this documentation. :bookmark_tabs:\n\nGet your Change standard template ready in MMS DEV. If you have it in MMS PROD you have to ask PCI to copy it in MMS DEV so you will have the same Id cross the environments.\n\nFollow the prerequisites and the branch strategy.\n\nTry to trigger your GitLab pipeline. :rocket:\n\nPrerequisites\n\nCreate a pipeline trigger token in your newly created project.\n\nTriggers the Standard change creation\n\nWith your token, trigger the pipeline .gitlab-ci.yml with a curl or an Invoke-RestMethod or any other HTTP client tool able to trigger a GitLab endpoint.\n\nTrough this API call, you'll be able to change (if you need) the empty variables in the read-only dictionary:\n\nimport Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem';\n\nDictionary\n\nDictionaries permit to match MMS items with NinGines pipelines. In your GitLab project you'll find a dictionary file in dictionaries folder:\n\n:::caution This dictionary is a read-only file to let you know the structure and what variables you can update thanks to your trigger. :::\n\nDescription\n\nFor the moment, you will be able to update the following variables in your Standard Change: - justification - change_technical_information\n\nmms_ci\n\nIf you need to update a specific field in your Standard Change, please create an issue in NinGines roadmap backlog.\n\nHow to find the Standard Change template ID\n\nThe ID is easily visible in the URL when you open a request. Example: `https://airliquide.service-now.com/nav_to.do?uri=%2Fu_adv_template_change.do%3Fsys_id%3D29e7a58cdb3aa950f1ad4662159619bc%26sysparm_record_target%3Du_adv_template%26sysparm_record_row%3D1%26sysparm_record_rows%3D1%26sysparm_record_list%3Du_tmpl_short_descriptionCONTAINSNinGines\n\n:::info Standard Change template ID: 29e7a58cdb3aa950f1ad4662159619bc :::\n\nBranch strategy\n\nIf you trigger your pipeline on the main branch the Change item will be created on MMS PROD environment. Use the develop branch to test the creation in MMS DEV environments.\n\nReturn status\n\nIn your project pipeline a notification will appear with the details regarding your Standard Change template creation:\n\n:::info [INFO] The change CHG0169646 has been created in MMS DEV (https://airliquidedev.service-now.com) :::""]","The significance of using MMS DEV and MMS PROD environments when triggering a pipeline is for testing and deployment purposes. If you trigger your pipeline on the main branch, the Change item will be created on the MMS PROD environment. Using the develop branch allows you to test the creation in the MMS DEV environment before deploying to production.","[{'source': 'cloud-services\\docs\\ningines\\03-interface\\change-creator-1-6.md', 'filename': 'cloud-services\\docs\\ningines\\03-interface\\change-creator-1-6.md'}]",simple,0.0
25,1,What are the different roles within GitLab Groups and what are their responsibilities?,"['## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## User Access Management\n\nCARM\n\nCARM is the sole source of truth knowing who is authorized to access GitLab (and other Air Liquide systems)\n\nIt allows a secure management of users. For example, once someone (employee, sub-contractor or business partner) leaves the company, CARM will make sure that all his/her access to GitLab is revoked. You, as a project manager or repo-owner, won\'t have to do it yourself in GitLab - admin peace of mind :)\n\n:::info\n\nAfter 90 days of inactivity, users are deactivated to reactivate their user, just connect to Gitlab url.\n\n:::\n\nWhen someone is removed from GitLab, his/her GitLab profile is set to ""Blocked"". After 12 months the user content will be moved to a system-wide ""Ghost User"" in order to maintain content for posterity.\n\nAuthorizations in GitLab\n\nTo delegate rights and permissions balancing security and client-autonomy, GIO has implemented a specific Groups structure in GitLab. You need to understand how it has been created and how it works before using the GitLab as a service. Developers as well as managers and project managers should read this tutorial.\n\nOverview\n\nWe provide a flexible structure for Groups GitLab, in which you can set your own organization.\n\n:::info\n\nGIO only defines and manages the Top Level of the Groups structure in GitLab and assigns Owners for each BIS/entity using the service. Underneath that level, each BIS is able to manage Groups and delegate permissions autonomously.\n\n:::\n\nRead through GitLab Permissions documentation for more information.\n\nGroups and Roles\n\nGIO creates and manages the top level Groups of the organization\n\nBIS Group Owners assign roles to people to subgroups and projects\n\nYou can manage all subgroups and projects within the Groups that you can access as per the permissions that were given to you by your Owners\n\nGroups and Sub-groups\n\nTop Level Groups: they reflect more or less the current BIS structure, with some adaptations. GIO will create them with you during the onboarding and after if necessary.\n\nSub-groups: they are set by you depending on your needs and constraints.\n\nRoles\n\nAdmin: this role is reserved solely for GIO.\n\nOwner: this user role has extended rights to manage all groups and sub-groups that he/she owns. GIO will assign this role to a limited number of people in the Top Level groups in the GitLab organization. Then, these persons will be able to create sub-groups and assign owners, maintainers and other roles to these sub-groups without the intervention of GIO.\n\nMaintainer: this user role can create projects within a Group\n\nDeveloper/Reporter/Guest: they are roles for contributors in your projects who don\'t need to manage the organization of groups or projects.\n\nRequest access to a Group\n\nWhen you are login for the first time in the GitLab you can only browse the Public/Internal projects and create a personal project.\n\n:::caution\n\nWe recommend that you only use personal projects for ...personal projects. All Air Liquide projects should reside into a BIS Group.\n\n:::\n\nFind the BIS Group where you or your project belongs to\n\nYou can either request access from the repo home page, if this option is enabled, or browse to the Members page of the Group to have the names of the Group Admin\n\nContact Group Owner(s) to request access']","There are several roles within GitLab Groups, each with different responsibilities. The 'Admin' role is reserved solely for GIO. The 'Owner' role has extended rights to manage all groups and sub-groups that he/she owns. GIO assigns this role to a limited number of people in the Top Level groups in the GitLab organization. These persons can then create sub-groups and assign owners, maintainers, and other roles to these sub-groups without GIO's intervention. The 'Maintainer' role can create projects within a Group. The 'Developer', 'Reporter', and 'Guest' roles are for contributors in projects who don't need to manage the organization of groups or projects.","[{'source': 'cloud-services\\docs\\sf\\03-gitlab\\04-user-access.md', 'filename': 'cloud-services\\docs\\sf\\03-gitlab\\04-user-access.md'}]",simple,2.0
26,2,"What is the appropriate course of action when you need AWS support for general questions about services and architecture topics, or when you have a P1 incident and need urgent assistance?","[""## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## AWS Team\n\nAWS Account Team\n\nAir Liquide is subscribed to the AWS Enterprise Support. It provides 24x7 technical support from high-quality engineers, tools and technology to automatically manage our environments, consultative architectural guidance delivered in the context of your applications and use-cases, and a designated Technical Account Manager (TAM) to coordinate access to proactive / preventative programs and AWS subject matter experts.\n\nAWS Role Name Contact email Contact Tel Scope Account Manager (AM) Edouard NOMME nomme@amazon.com +33 6 28 54 67 11 EMEA Technical Account Manager (TAM) RÃ©mi DURANCET dduranc@amazon.fr +33 (0) 7 71 44 41 06 EMEA Solutions Architect (SA) Olivier ROSSANT orossant@amazon.fr +33 (0) 6 88 13 51 13 EMEA Customer Solutions Manager (CSM) Omar ZRAIBI omarzr@amazon.fr +33 (0) 6 59 63 09 81 EMEA Conciergerie Ann HESTERMANN maryjeah@amazon.com - EMEA Central Sales Representative (CSR) Alexandre RUSCART ruscarta@amazon.com - EMEA Account Manager (AM) Donielle NYLAND ndoniell@amazon.com - AMER Solutions Architect Ewanlen ATIVIE mativie@amazon.com - AMER Customer Solutions Manager (CSM) Danilo LIBERZTO danliber@amazon.fr - AMER Account Manager Vimalraj SELVARAJ msvimalr@amazon.com - APAC Solutions Architect (SA) Keith ROZARIO krozario@amazon.com - APAC Business Developer (BD) Jingxian SUN suhingxi@amazon.com - China Solutions Architect (SA) Phil ZHAO hongz@amazon.com +86-137-6461-6419 China\n\nThe AWS account team can be reached via the following distribution lists: * AWS TAM (Global): aws-airliquide-tams@amazon.com * AWS team (EMEA): aws-airliquide-emea@amazon.com * AWS team (Global): aws-airliquide-emea@amazon.com\n\nAll AWS member have -sc@airliquide.com accounts and can be reached via Google Chat.\n\nHow to contact the team\n\nYou need support for general questions about services and architecture topics: open a case to the support and/or directly contact your region's SA and TAM\n\nYou have a question about the AWS Roadmap or want to submit a PFR: contact the TAM\n\nYou want more details about incentives, migration programs: contact your region's Account Manager\n\nYou have a question or issue about billing topics: contact the AWS Concierge\n\nYou need a follow up on operational and billing topics: contact teh Customer Solutions Manager\n\nYou have a P1 incident and need urgent support from AWS: open a ticket to the AWS support with the appropriate severity level. The TAM* will be paged and will onboard timely to coordinate the AWS people to work towards the resolution\n\nThe AWS Team has a Google Chat account and can be reached via this channel if necessary.""]","When you need AWS support for general questions about services and architecture topics, you should open a case to the support and/or directly contact your region's Solutions Architect (SA) and Technical Account Manager (TAM). If you have a P1 incident and need urgent assistance, open a ticket to the AWS support with the appropriate severity level. The Technical Account Manager (TAM) will be paged and will onboard timely to coordinate the AWS people to work towards the resolution.","[{'source': 'cloud-services\\docs\\aws-managed\\01-intro\\01-aws-team.md', 'filename': 'cloud-services\\docs\\aws-managed\\01-intro\\01-aws-team.md'}]",simple,4.0
27,3,Why is it important to update the Operations contact details for Landing Zone automations warnings?,"['## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## Update your accounts Operations Contact details\n\nIn order to make it easier for you to have operational information about your account, we are requesting you to update the Operations contact details under Alternate Contacts of your account information.\n\nWhy update?\n\nIt is very important that you do this update:\n\nYou will receive important information from AWS (instance health status, certificates renewal, outages...)\n\nGIO Cloud Services has stopped forwarding AWS operations emails starting February 1st, 2020 onwards.\n\n:::warning\n\nThis information is used by Landing Zone automations to warn you about upcoming actions that will impact your resources. Make sure it is accurate and up to date to avoid missing these important notifications.\n\n:::\n\nProcedure\n\nLog into your account with the federation access and using an admin profile\n\nGo to the My Account section of the console ()\n\nModify the operations contact as shown below:\n\nClick on the Edit button\n\nIndicate a contact. It should be Distribution List. You must avoid to set a single person email.']",It is important to update the Operations contact details for Landing Zone automations warnings because this information is used to warn you about upcoming actions that will impact your resources. Ensuring the contact details are accurate and up to date will help you avoid missing these important notifications.,"[{'source': 'cloud-services\\docs\\aws-lab-essential\\08-tutorials\\04-console\\02-tuto-operations-contact.md', 'filename': 'cloud-services\\docs\\aws-lab-essential\\08-tutorials\\04-console\\02-tuto-operations-contact.md'}]",simple,6.0
28,4,"What's the process for getting a VPS, from request to catalog search?","['## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## Virtual Private Server\n\nHow to request a VM ?\n\ni.e request a(some) EC2(s) within this offer ?\n\nGo directly to the the request Provisioning a Virtual Private Server\n\nOR\n\nGo into ServiceNow and then\n\n""Request Catalog - Catalogs""\n\nand search the request\n\nGIO Services\n\nBusiness Apps Foundations\n\nAdvanced Cloud Services\n\nProvisioning a Virtual Private Server\n\nHow to request access to the VM ?\n\nFor a Windows Servers\n\nA special tag gio:provision:option is set to autoGroupId.\n\nDuring the provisioning of the VM, a dedicated Active Directory group will be created:\n\nDGG_Cloud-Services_-_SM\n\nwhere - is the ID of the AWS account in which the server has been provisoned. The account ID can be found in your request (it depends on the HUB and if it is for IT or OT): check the Account Id (not the account name) under ""IT account"" or ""OT account"" in the request. - is the IP address of the server in hexadecimal\n\nYou can use this request in the service catalog ""Active Directory Add / remove Active Directory Groups Members"" to request to add a user in an Active Directory Group.\n\nOr you can find the request in Service Now catalog:\n\nGIO Services\n\nBusiness Apps Foundations\n\nIdentity & Access Management\n\nActive Directory Add / remove Active Directory Groups Members\n\nFor a Linux Servers\n\nYou should use the request in Service Now service catalog ""Manage access to unix/Linux servers - All HUBs"" to request the ctr or the ctu access to the server.\n\nOr you can find the request in Service Now catalog:\n\nGIO Services\n\nBusiness Apps Foundations\n\nHybrid Cloud\n\nIaas VM (Windows, Linux)\n\nManage access to unix/Linux servers - All HUBs']","To request a Virtual Private Server (VPS), you can either go directly to the request 'Provisioning a Virtual Private Server' or go to ServiceNow, then to 'Request Catalog - Catalogs' and search for the request 'Provisioning a Virtual Private Server'. For access to a Windows Server, a special tag 'gio:provision:option' is set to 'autoGroupId'. During the provisioning of the VM, a dedicated Active Directory group is created. You can use the request in the service catalog 'Active Directory Add / remove Active Directory Groups Members' to request to add a user in an Active Directory Group. For access to a Linux Server, you should use the request in ServiceNow service catalog 'Manage access to unix/Linux servers - All HUBs' to request the ctr or the ctu access to the server.","[{'source': 'cloud-services\\docs\\virtual-private-server\\04-how-to\\index.md', 'filename': 'cloud-services\\docs\\virtual-private-server\\04-how-to\\index.md'}]",reasoning,8.0
29,5,What's the process to adjust AWS Lab's monthly alert trigger?,"['## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## Alerting\n\nDefault Budget Alerts provided by GIO\n\nGIO is providing you with standard tooling to be alerted when your consumption reaches a threshold that you define. When creating your AWS Account, we deploy a default Billing Alarm that is based on the monthly threshold (in USD) that you have indicated in the request. The alert is based on the CloudWatch Billing Alarm feature deployed in the us-east-1 region.\n\nTo receive email alerts you must have subscribed to the SNS Topic for this alarm. You should have received an email from Billing Alert (no-reply@sns.amazonaws.com via amazonses.com) similar to the one below. If not, please check your spam.\n\nClick on Confirm Subscription. It will open a new window in your web browser indicating that you have successfully subscribed to the SNS topic.\n\nYou will now receive an email if your consumption during a given month reaches the billing alert threshold.\n\nModify the alarm threshold\n\nIf you wish to readjust the alarm threshold:\n\nLog in to the AWS Console for your Lab Account\n\nSwitch to the us-east-1 region (billing alarm can only be set in the us-east-1 region)\n\nBrowse to the CloudWatch Console and click on Billing in the left pane\n\nSelect the alarm and in Actions select Modify\n\nSet the new threshold amount and click âSave changesâ']","To adjust the AWS Lab's monthly alert trigger, follow these steps: 1. Log in to the AWS Console for your Lab Account. 2. Switch to the us-east-1 region, as the billing alarm can only be set in this region. 3. Browse to the CloudWatch Console and click on Billing in the left pane. 4. Select the alarm and in Actions select Modify. 5. Set the new threshold amount and click 'Save changes'.","[{'source': 'cloud-services\\docs\\aws-lab-essential\\05-finops\\08-alerting.md', 'filename': 'cloud-services\\docs\\aws-lab-essential\\05-finops\\08-alerting.md'}]",reasoning,10.0
30,6,"What does GCP Standard Support offer for cloud management, including tech assistance and optimization tips, and how to access it during French business hours via Google Cloud Console?","[""## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## Support\n\nGIO Support\n\nGIO Cloud Services Support for GCP Essential services is available during French business days, from 9 AM to 6 PM CET/CEST, including French Holidays. Please note that there is no specific Service Level Agreement (SLA) beyond these support hours. The support during these hours covers incidents of all priorities and is managed by Europe team to serve a worldwide scope.\n\nGCP Standard Support\n\nGCP Standard support is available for all project.\n\nGCP Standard support experts are available for many types of requests regarding GCP services, architecture, etc.:\n\nTechnical Support Service: This is the actual service provided by the support team to help resolve technical problems. This could involve answering technical queries, providing guidance, or troubleshooting issues\n\nActive assist recommendations: Receive recommendations and insights to help optimize your Google Cloud resources for cost, security, performance, and manageability.\n\nOpen a case to the GCP Standard Support from the Console\n\nSign in to the Google Cloud console Support page\n\nSelect the project for which you'd like to open a support case\n\nOpen Cases\n\nClick Create case\n\nComplete the required fields and submit the form\n\n:::tip More info on how to manage support cases: https://cloud.google.com/support/docs/manage-cases :::""]","GCP Standard Support offers technical assistance through its Technical Support Service, which helps resolve technical problems by answering queries, providing guidance, and troubleshooting issues. It also provides Active Assist recommendations to optimize Google Cloud resources for cost, security, performance, and manageability. To access this support during French business hours via Google Cloud Console, sign in to the Google Cloud Console Support page, select the project for which you'd like to open a support case, click on 'Open Cases', then 'Create case', complete the required fields and submit the form. However, the context does not specify if this support is available during French business hours.","[{'source': 'cloud-services\\docs\\gcp-essential\\02-quick-start\\02-support.md', 'filename': 'cloud-services\\docs\\gcp-essential\\02-quick-start\\02-support.md'}]",multi_context,15.0
