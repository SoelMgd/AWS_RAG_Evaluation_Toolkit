{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate your RAG\n",
    "\n",
    "This notebook aims to evaluate your RAG built with `llama_index`, which uses documents stored in an AWS S3 bucket. The `RAGConfig` class allows you to specify:\n",
    "\n",
    "- A HuggingFace embedding model\n",
    "- An LLM available on AWS BedrockChat\n",
    "- A prompt\n",
    "\n",
    "You can experiment with different configurations to find the most effective ones. \n",
    "\n",
    "Use the `RAGEvaluate` class to assess these configurations using various metrics. Under the hood, `RAGEvaluate` employs `RAGAS`.\n",
    "\n",
    "Save your results and compare them using the `AnalyzeResults` class to visualize the score distributions across different metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import s3fs\n",
    "import nest_asyncio\n",
    "\n",
    "from Core.rag_config import RAGConfig\n",
    "from Core.rag_evaluate import RAGEvaluate\n",
    "from Core.results_analyzer import ResultsAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import your AWS credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../aws_credentials.cfg')\n",
    "\n",
    "aws_access_key_id = config.get('default', 'aws_access_key_id')\n",
    "aws_secret_access_key = config.get('default', 'aws_secret_access_key')\n",
    "region_name = config.get('default', 'region_name')\n",
    "\n",
    "# Session boto3 with credentials\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    region_name=region_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your S3 bucket with the documents for the RAG in AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)\n",
    "s3_bucket_name = \"airliquide-alit-gio-aiops-dev/chaima/indexes_md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test different configurations\n",
    "\n",
    "Examples of HuggingFace embedding models to try:\n",
    "- \"BAAI/bge-small-en-v1.5\"\n",
    "- \"avsolatorio/GIST-small-Embedding-v0\"\n",
    "- \"avsolatorio/NoInstruct-small-Embedding-v0\"\n",
    "\n",
    "Examples of AWS Bedrock LLM to try:\n",
    "- \"mistral.mistral-large-2402-v1:0\"\n",
    "- \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "- \"anthropic.claude-v2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import answer_relevancy, faithfulness, context_precision, context_recall\n",
    "\n",
    "prompt_basic = \"You are a chatbot powered by AIops Team called YodaAI, able to have a normal interactions, as well as talk about the CS (Cloud Services) Tech Guides given in the context. The CS Tech Guides is a documentation for CS service employees in GIO (Global Infrastructure Operations) entity in AirLiquide. Introduce yourself based on the presentation in this system prompt if there is not a clear a question. If you can't find the response in the context please return this answer: Unfortunately I don't have any information about [topic of the query] in the context that was provided to me. As an AI assistant without access to external information, I can only discuss the details contained in the CS Tech Guides\"\n",
    "\n",
    "config1 = RAGConfig(name = \"mistral_large_avsolatorio_non_instruct_small\",\n",
    "                   generation_llm=\"mistral.mistral-large-2402-v1:0\",\n",
    "                   embedder=\"avsolatorio/NoInstruct-small-Embedding-v0\",\n",
    "                   prompt=prompt_basic,\n",
    "                   scoring_llm=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "                   metrics= [faithfulness,answer_relevancy, context_precision, context_recall])\n",
    "\n",
    "config2 = RAGConfig(name = \"claude3_BGE_small\",\n",
    "                   generation_llm=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "                   embedder=\"BAAI/bge-small-en-v1.5\",\n",
    "                   prompt=prompt_basic,\n",
    "                   scoring_llm=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "                   metrics= [faithfulness,answer_relevancy, context_precision, context_recall])\n",
    "\n",
    "config3 = RAGConfig(name = \"claude2_BGE_small\",\n",
    "                   generation_llm=\"anthropic.claude-v2\",\n",
    "                   embedder=\"BAAI/bge-small-en-v1.5\",\n",
    "                   prompt=prompt_basic,\n",
    "                   scoring_llm=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "                   metrics= [faithfulness,answer_relevancy, context_precision, context_recall])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate your configurations\n",
    "\n",
    "Import your testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = pd.read_csv('Testsets/RAG_testset_processed.csv')\n",
    "testset = testset.iloc[:2,:]\n",
    "testset.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate your config with RAGEvaluate and RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RAGEvaluate(config1, session, s3, s3_bucket_name)\n",
    "await evaluator.evaluate(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results for benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.save_results_to_json('Results/Mistral_Large_avsolatorio_non_instruct_small.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare your configurations\n",
    "\n",
    "Load your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = ResultsAnalyzer(['Results/Mistral_Large_GIST.json', 'Results/Mistral_Large_avsolatorio_non_instruct_small.json'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.report(threshold_dict={'faithfulness':0.9})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.show_heatmap(metric='faithfulness')\n",
    "analyzer.show_heatmap(metric='answer_relevancy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot barplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.barplot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragvenv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
